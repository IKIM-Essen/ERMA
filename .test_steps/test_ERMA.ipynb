{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src= \"ERMA_workflow.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Step similarity search\n",
    "# Input: fasta files\n",
    "# Output: tabular similarity search results\n",
    "# commands:\n",
    "# usearch vs silva_v138.2 database (510495 reads): usearch -usearch_local {input.fasta} -db {input.silva} -blast6out {output.silva_results} -evalue 1e-5 -threads {params.internal_threads} -strand plus -mincols 200 2> {log}\n",
    "# diamond vs card_v3.3.0 database (4840 reads): diamond blastx -d {input.card} -q {input.fasta} -o {output.card_results} --outfmt 6 --evalue 1e-5 --quiet --threads {params.internal_threads} 2> {log}\n",
    "# Notes: Many rules that prepare the similarity search are reproduced with simple bash commands\n",
    "\n",
    "import subprocess\n",
    "import pathlib, os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "github = base / \".github\"\n",
    "\n",
    "silva_dir = github / \"data/silva_db\"\n",
    "card_dir = github / \"data/card_db\"\n",
    "fastq_dir = github / \"data/fastq\"\n",
    "test_out = base / \".test_steps\"\n",
    "\n",
    "fastq = fastq_dir / \"test_epic_data.fastq.gz\"\n",
    "fasta = fastq.with_suffix(\".fasta\")\n",
    "silva_gz = silva_dir / \"sub_silva_seq_RNA.fasta.gz\"\n",
    "silva_fa = silva_gz.with_suffix(\"\")\n",
    "translated_silva = silva_fa.with_name(silva_fa.name.replace(\"_RNA\", \"\"))\n",
    "card_tar = card_dir / \"card_seq.tar.bz2\"\n",
    "card_fasta = card_dir / \"protein_fasta_protein_homolog_model.fasta\"\n",
    "card_db = card_dir / \"card_db.dmnd\"\n",
    "result_dir = test_out / \"results\"\n",
    "card_results = result_dir / \"card_results.txt\"\n",
    "silva_results = result_dir / \"SILVA_results.txt\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd, silent=False):\n",
    "    result = subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL if silent else None, stderr=subprocess.DEVNULL if silent else None)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "\n",
    "def count_lines(file, pattern=None):\n",
    "    cmd = f\"grep -c '{pattern}' {file}\" if pattern else f\"wc -l < {file}\"\n",
    "    return int(subprocess.check_output(cmd, shell=True))\n",
    "\n",
    "def clean(folder, keep):\n",
    "    for item in Path(folder).iterdir():\n",
    "        if item.name not in keep:\n",
    "            if item.is_file():\n",
    "                item.unlink()\n",
    "            elif item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "\n",
    "# === Prepare and run similarity search ===\n",
    "run(f\"mkdir -p {result_dir}\")\n",
    "run(f\"seqtk seq -a {fastq} > {fasta}\")\n",
    "run(f\"gzip -dk {silva_gz}\")\n",
    "run(f\"seqtk seq -r {silva_fa} > {translated_silva}\")\n",
    "run(f\"tar -xjf {card_tar} -C {card_dir}\")\n",
    "run(f\"diamond makedb --in {card_fasta} -d {card_db.with_suffix('')}\")\n",
    "run(f\"diamond blastx -d {card_db} -q {fasta} -o {card_results} --outfmt 6 --evalue 1e-5 --threads 1 --quiet\")\n",
    "run(f\"usearch -usearch_local {fasta} -db {translated_silva} -blast6out {silva_results} -evalue 1e-5 -threads 1 -strand plus -mincols 200 > /dev/null 2>&1\", silent=True)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\nsample,state,total_count\")\n",
    "print(f\"Number of FastQ input reads,{count_lines(fasta, '^>')}\")\n",
    "print(f\"Diamond output hits,test,{count_lines(card_results)}\")\n",
    "print(f\"Usearch output hits,test,{count_lines(silva_results)}\")\n",
    "\n",
    "# === Cleanup ===\n",
    "clean(card_dir, {card_tar.name})\n",
    "for f in fastq_dir.glob(\"*.fasta\"): f.unlink()\n",
    "for f in silva_dir.glob(\"*.fasta\"): f.unlink()\n",
    "\n",
    "# === Report ===\n",
    "display(Markdown(f\"### Processing Complete\\n- CARD hits: `{count_lines(card_results)}`\\n- SILVA hits: `{count_lines(silva_results)}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Integrate similarity search results\n",
    "# Selfwritten python script \"integrate_blast_data.py\"\n",
    "# Input: diamond, usearch results, ARO Mapping file\n",
    "# Output: Processed integrated search results\n",
    "\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os, pathlib, subprocess\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "github = base / \".github\"\n",
    "log_dir = base / \"logs\"\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "silva_dir = github / \"data/silva_db\"\n",
    "card_dir = github / \"data/card_db\"\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "silva_res = result_dir / \"SILVA_results.txt\"\n",
    "card_res = result_dir / \"card_results.txt\"\n",
    "aro_file = \"aro_index.tsv\"\n",
    "aro_path = card_dir / \"aro_index.tsv\"\n",
    "aro_tar = card_dir / \"card_seq.tar.bz2\"\n",
    "card_interm = result_dir / \"card_intermed.csv\"\n",
    "silva_interm = result_dir / \"silva_intermed.csv\"\n",
    "result = result_dir / \"integrated_result.csv\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd):\n",
    "    result = subprocess.run(cmd, shell=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "\n",
    "# === Extract Aro ===\n",
    "run(f\"tar -xvjf {aro_tar} ./{aro_file}; mv {aro_file} {card_dir}\")\n",
    "\n",
    "# === Integrate Script ===\n",
    "def process_card_results(card_path, aro_path, blast_columns, output_path):\n",
    "    \"\"\"Process CARD results and save them to an intermediate output file\"\"\"\n",
    "    aro_df = pd.read_csv(aro_path, sep=\"\\t\")\n",
    "\n",
    "    with open(card_path, \"rt\") as f_in, open(output_path, \"w\") as f_out:\n",
    "        card_df = pd.read_csv(f_in, sep=\"\\t\", names=blast_columns)\n",
    "        card_df[\"part\"] = \"ABR\"\n",
    "        # Extract ARO accession (formatted like: ARO|...|ACCESSION|...)\n",
    "        card_df[\"ARO Accession\"] = card_df[\"subject_id\"].str.split(\n",
    "            \"|\", expand=True\n",
    "        )[2]\n",
    "        merged_df = card_df.merge(aro_df, on=\"ARO Accession\", how=\"left\")\n",
    "        merged_df.to_csv(f_out, index=False)\n",
    "\n",
    "\n",
    "def process_silva_results(silva_path, blast_columns, output_path):\n",
    "    \"\"\"Process SILVA results and save them to an intermediate output file.\"\"\"\n",
    "\n",
    "    with open(silva_path, \"rt\") as f_in, open(output_path, \"w\") as f_out:\n",
    "        silva_df = pd.read_csv(f_in, sep=\"\\t\", names=blast_columns)\n",
    "        silva_df[\"part\"] = \"16S\"\n",
    "        # Extract the primary accession (before '.') from SILVA subject_id\n",
    "        silva_df[\"primaryAccession\"] = silva_df[\"subject_id\"].str.split(\n",
    "            \".\", expand=True\n",
    "        )[0]\n",
    "        silva_df[\"genus\"] = silva_df[\"subject_id\"].str.split(\";\").str[-2]\n",
    "        silva_df.to_csv(f_out, index=False)\n",
    "\n",
    "\n",
    "def merge_results(card_output, silva_output, final_output):\n",
    "    \"\"\"Merge processed CARD and SILVA results into one final output file and update overview\"\"\"\n",
    "    card_df = pd.read_csv(card_output)\n",
    "    silva_df = pd.read_csv(silva_output)\n",
    "\n",
    "    combined_df = pd.concat([silva_df, card_df])\n",
    "    combined_df.to_csv(final_output, index=False)\n",
    "\n",
    "    # Count number of rows in the combined DataFrame\n",
    "    count = len(combined_df)\n",
    "\n",
    "    print(f\"Merged similarity hits,{count}\\n\")\n",
    "\n",
    "blast_columns = [\n",
    "    \"query_id\",\n",
    "    \"subject_id\",\n",
    "    \"perc_identity\",\n",
    "    \"align_length\",\n",
    "    \"mismatches\",\n",
    "    \"gap_opens\",\n",
    "    \"q_start\",\n",
    "    \"q_end\",\n",
    "    \"s_start\",\n",
    "    \"s_end\",\n",
    "    \"evalue\",\n",
    "    \"bit_score\",\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_card = executor.submit(\n",
    "        process_card_results, card_res, aro_path, blast_columns, card_interm\n",
    "    )\n",
    "    future_silva = executor.submit(\n",
    "        process_silva_results, silva_res, blast_columns, silva_interm\n",
    "    )\n",
    "\n",
    "    future_card.result()\n",
    "    future_silva.result()\n",
    "\n",
    "merge_results(card_interm, silva_interm, result)\n",
    "\n",
    "# === Cleanup ===\n",
    "run(f\"rm {result_dir}/*intermed*; rm {card_dir}/aro*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filter Blast results\n",
    "# Selfwritten python script \"filter_blast_results.py\"\n",
    "# Input: integrated_filtered_results.csv\n",
    "# Output: filtered_results.csv\n",
    "\n",
    "import pandas as pd\n",
    "import os, pathlib, subprocess\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "merge_result = result_dir / \"integrated_result.csv\"\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd):\n",
    "    result = subprocess.run(cmd, shell=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "    \n",
    "# === Filter Script ===\n",
    "dtype_dict = {\n",
    "    \"query_id\": \"string\",\n",
    "    \"perc_identity\": \"float\",\n",
    "    \"align_length\": \"int\",\n",
    "    \"evalue\": \"float\",\n",
    "    \"part\": \"string\",\n",
    "    \"genus\": \"string\",\n",
    "    \"AMR Gene Family\": \"string\",\n",
    "}\n",
    "\n",
    "\n",
    "def read_input_data(input_file):\n",
    "    \"\"\"Load relevant columns from input file with proper dtypes\"\"\"\n",
    "    return pd.read_csv(input_file, sep=\",\", dtype=dtype_dict, usecols=dtype_dict.keys())\n",
    "\n",
    "\n",
    "def filter_by_identity(df, part, min_similarity):\n",
    "    \"\"\"Filter BLAST result for either ABR and 16S part based on percent identity\"\"\"\n",
    "    data_pre = df[df[\"part\"] == part]\n",
    "    filtered = data_pre[data_pre[\"perc_identity\"] > min_similarity * 100]\n",
    "    filtered_count = len(data_pre) - len(filtered)\n",
    "    return filtered, filtered_count\n",
    "\n",
    "\n",
    "def keep_max_identity_per_query(df):\n",
    "    \"\"\"For each query_id, keep only rows with the highest percent identity\"\"\"\n",
    "    max_identities = df.groupby(\"query_id\")[\"perc_identity\"].max().reset_index()\n",
    "    merged = df.merge(max_identities, on=[\"query_id\", \"perc_identity\"])\n",
    "    return merged\n",
    "\n",
    "def keep_best_per_query(df):\n",
    "    \"\"\"For each query_id, keep the row with the highest perc_identity and lowest evalue\"\"\"\n",
    "    return (\n",
    "        df.sort_values(\n",
    "            by=[\"query_id\"] + [\"perc_identity\", \"evalue\"], \n",
    "            ascending=[True,False, True]\n",
    "            ).drop_duplicates(subset=\"query_id\", keep=\"first\")\n",
    "    )\n",
    "\n",
    "def clean_16s_query_ids(df):\n",
    "    \"\"\"Remove anything after the first whitespace in 16S query IDs\"\"\"\n",
    "    df[\"query_id\"] = df[\"query_id\"].str.split().str[0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_parts_on_query_id(abr_data, s16_data):\n",
    "    \"\"\"Return only rows with query_ids present in both ABR and 16S data\"\"\"\n",
    "    common_ids = pd.Index(abr_data[\"query_id\"]).intersection(s16_data[\"query_id\"])\n",
    "    return (\n",
    "        abr_data[abr_data[\"query_id\"].isin(common_ids)],\n",
    "        s16_data[s16_data[\"query_id\"].isin(common_ids)],\n",
    "    )\n",
    "\n",
    "def write_summary(sample, stats):\n",
    "    \"\"\"Write all filtering summary statistics to the overview file\"\"\"\n",
    "    if overview_table.is_file():\n",
    "        for stat_name, value in stats.items():\n",
    "            print(f\"{sample},{stat_name},{value}\")\n",
    "    else:\n",
    "        with open(overview_table, \"a\") as file:\n",
    "            for stat_name, value in stats.items():\n",
    "                file.write(f\"{sample},{stat_name},{value}\\n\")\n",
    "                print(f\"{sample},{stat_name},{value}\")        \n",
    "\n",
    "def rename_for_merge(df,part):\n",
    "    df_renamed = df.rename(columns={\n",
    "        \"perc_identity\": \"perc_identity_\"+part,\n",
    "        \"align_length\": \"align_length_\"+part,\n",
    "        \"evalue\": \"evalue_\"+part,\n",
    "    })\n",
    "    return df_renamed\n",
    "\n",
    "def filter_blast_results(input_file, output_file, min_similarity):\n",
    "    \"\"\"Main filtering logic for BLAST results across ABR and 16S data parts\"\"\"\n",
    "    df = read_input_data(input_file)\n",
    "\n",
    "    # ABR filtering\n",
    "    abr_threshold_filtered, abr_removed_identity = filter_by_identity(df, \"ABR\", min_similarity)\n",
    "    abr_best_identity = keep_max_identity_per_query(abr_threshold_filtered)\n",
    "    abr_best_query = keep_best_per_query(abr_best_identity)\n",
    "    abr_final = rename_for_merge(abr_best_query ,\"ABR\")\n",
    "    abr_removed_max = len(abr_threshold_filtered) - len(abr_final)\n",
    "\n",
    "    # 16S filtering\n",
    "    s16_threshold_filtered, s16_removed_identity = filter_by_identity(df, \"16S\", min_similarity)\n",
    "    s16_cleaned = clean_16s_query_ids(s16_threshold_filtered)\n",
    "    s16_best_identity = keep_max_identity_per_query(s16_cleaned)\n",
    "    s16_best_query = keep_best_per_query(s16_best_identity)\n",
    "    s16_final = rename_for_merge(s16_best_query,\"16S\")\n",
    "    s16_removed_max = len(s16_threshold_filtered) - len(s16_final)\n",
    "\n",
    "    # Match ABR and 16S by query_id\n",
    "    abr_common, s16_common = merge_parts_on_query_id(abr_final, s16_final)\n",
    "    removed_query_id_mismatch = (len(abr_final) + len(s16_final)) - (\n",
    "        len(abr_common)\n",
    "    )\n",
    "\n",
    "    # Merge side-by-side on query_id\n",
    "    merged = pd.merge(\n",
    "        abr_final[[\"query_id\", \"AMR Gene Family\", \"perc_identity_ABR\", \"align_length_ABR\", \"evalue_ABR\"]],\n",
    "        s16_final[[\"query_id\", \"genus\", \"perc_identity_16S\", \"align_length_16S\", \"evalue_16S\"]],\n",
    "        on=\"query_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    merged.to_csv(output_file, index=False)\n",
    "\n",
    "    # Extract sample and part from file path\n",
    "    sample = \"test_epic_data\"\n",
    "\n",
    "    # Write summary\n",
    "    stats = {\n",
    "        \"Diamond hits < similarity threshold\": \"-\" + str(abr_removed_identity),\n",
    "        \"Diamond hits NOT highest percentage identity per query\": \"-\" + str(abr_removed_max),\n",
    "        \"Usearch hits < similarity threshold\": \"-\" + str(s16_removed_identity),\n",
    "        \"Usearch hits NOT highest percentage identity per query\": \"-\" + str(s16_removed_max),\n",
    "        \"Query hit in only one of two databases\": \"-\" + str(removed_query_id_mismatch),\n",
    "        \"Filtered fusion reads\": len(merged),\n",
    "    }\n",
    "    write_summary(sample, stats)\n",
    "\n",
    "filter_blast_results(merge_result, filter_result, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create abundance table\n",
    "# Selfwritten python script \"generate_genus_distribution_table.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: abundance plot over all ABRs\n",
    "\n",
    "import pandas as pd\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "\n",
    "# === Abundance Table Script ===\n",
    "\n",
    "def process_combined_data(combined_data, sample_name):\n",
    "    combined_data[\"sample\"] = sample_name\n",
    "\n",
    "    # Count genus occurrences per AMR Gene Family\n",
    "    genus_counts = (\n",
    "        combined_data.groupby([\"sample\", \"AMR Gene Family\", \"genus\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"genus_count\")\n",
    "    )\n",
    "\n",
    "    # Calculate total genus count per AMR Gene Family within each sample\n",
    "    total_counts = (\n",
    "        genus_counts.groupby([\"sample\", \"AMR Gene Family\"])[\"genus_count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_count\")\n",
    "    )\n",
    "\n",
    "    # Join and calculate relative abundance\n",
    "    result = pd.merge(genus_counts, total_counts, on=[\"sample\", \"AMR Gene Family\"])\n",
    "    result[\"relative_genus_count\"] = round(\n",
    "        result[\"genus_count\"] / result[\"total_count\"], 4\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def load_and_merge_parts(file_list):\n",
    "    \"\"\"Load and merges dataframes from compressed CSV files\"\"\"\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file due to read error [{file}]: {repr(e)}\")\n",
    "    if data_frames:\n",
    "        merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        merged_df = pd.DataFrame()\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def export_genera_abundance(input_files, output_path):\n",
    "    \"\"\"Group input files by sample\"\"\"\n",
    "    sample_to_files = {}\n",
    "    for file in [str(input_files)]:\n",
    "        # Extract sample name from the file path, assuming 3rd-to-last split is the sample name\n",
    "        sample = \"test_epic_data\"\n",
    "        sample_to_files.setdefault(sample, []).append(file)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for sample_name, files in sample_to_files.items():\n",
    "        merged_data = load_and_merge_parts(files)\n",
    "        sample_data = process_combined_data(merged_data, sample_name)\n",
    "        all_data.append(sample_data)\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=[\"sample\",\"AMR Gene Family\",\"genus_count\"], ascending=False)\n",
    "\n",
    "    # Export the final aggregated data to a CSV file\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    display(final_df)\n",
    "\n",
    "export_genera_abundance(filter_result, abundance_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create stacked bar abundance plot\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: abundance file\n",
    "# Output: bubble plot per sample\n",
    "\n",
    "import os, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "abundance_bar_plot = result_dir / \"combined_genus_abundance_barplot.html\"\n",
    "\n",
    "# ─── Constants ─────────────────────────────────────────────────────────\n",
    "RESERVED_COLOR = 'rgb(217,217,217)'\n",
    "AMR_MIN_FRACTION = 0.01\n",
    "\n",
    "def get_genus_colors(all_genera):\n",
    "    \"\"\"Assign consistent, distinguishable colors to each genus.\"\"\"\n",
    "    top_colors = [\n",
    "        '#D62728',  # dark red\n",
    "        '#FF7F0E',  # orange\n",
    "        '#8B4513',  # brown\n",
    "        '#1F77B4',  # dark blue\n",
    "        '#800080',  # purple\n",
    "        '#7F7F7F',  # gray\n",
    "        '#2CA02C',  # dark green\n",
    "        '#1E90FF',  # blue\n",
    "        '#BA55D3',  # medium orchid\n",
    "        '#BCBD22',  # yellow-green\n",
    "    ]\n",
    "\n",
    "    fallback_palette = (\n",
    "        px.colors.qualitative.Pastel +\n",
    "        px.colors.qualitative.Set3 +\n",
    "        px.colors.qualitative.Alphabet +\n",
    "        px.colors.qualitative.Light24 +\n",
    "        px.colors.qualitative.Bold\n",
    "    )\n",
    "\n",
    "    # Remove duplicates and reserved color from palette\n",
    "    color_pool = list(dict.fromkeys(top_colors + fallback_palette))\n",
    "    if RESERVED_COLOR in color_pool:\n",
    "        color_pool.remove(RESERVED_COLOR)\n",
    "\n",
    "    # Assign genera with a unique color each\n",
    "    genus_list = [g for g in all_genera if g != \"Others\"]\n",
    "    if len(genus_list) > len(color_pool):\n",
    "        raise ValueError(f\"Too many genera ({len(genus_list)}) for available color pool.\")\n",
    "    genus_colors = {g: color_pool[i] for i, g in enumerate(genus_list)}\n",
    "    genus_colors[\"Others\"] = RESERVED_COLOR\n",
    "    return genus_colors\n",
    "\n",
    "def preprocess_abundance(df, amr, min_genus_abundance, force_include, force_exclude):\n",
    "    \"\"\"Filter and aggregate genus abundance data for a given AMR family.\"\"\"\n",
    "    df_amr = df[df[\"AMR Gene Family\"] == amr].copy()\n",
    "\n",
    "    # Determine low-abundance or excluded genera\n",
    "    low_abundance = df_amr[\n",
    "        ((df_amr[\"relative_genus_count\"] <= min_genus_abundance) & (~df_amr[\"genus\"].isin(force_include))) |\n",
    "        (df_amr[\"genus\"].isin(force_exclude))\n",
    "    ]\n",
    "    others = (\n",
    "        low_abundance.groupby(['sample', 'total_count'], as_index=False)\n",
    "        .agg({\"relative_genus_count\": \"sum\"})\n",
    "        .assign(genus=\"Others\")\n",
    "    )\n",
    "    others[\"sample_label\"] = others[\"sample\"] + \" (\" + others[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    # Remove excluded genera\n",
    "    df_amr = df_amr[~df_amr[\"genus\"].isin(force_exclude)]\n",
    "    df_amr = df_amr.sort_values(by=['sample','AMR Gene Family','genus_count'],ascending=[True,False,False])\n",
    "    # plot high abundance or forced-includes\n",
    "    df_amr_filtered = df_amr[\n",
    "        (df_amr[\"relative_genus_count\"] > min_genus_abundance) | (df_amr[\"genus\"].isin(force_include))\n",
    "    ]\n",
    "\n",
    "    # Add \"Others\"\n",
    "    df_final = pd.concat([df_amr_filtered, others], ignore_index=True)\n",
    "    df_final[\"sample_label\"] = df_final[\"sample\"] + \" (\" + df_final[\"total_count\"].astype(str) + \")\"\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def plot_stacked_abundance(\n",
    "    observed_csv,\n",
    "    output_html,\n",
    "    min_genus_abundance,\n",
    "    force_include=None,\n",
    "    force_exclude=None,\n",
    "    min_reads=20\n",
    "):\n",
    "\n",
    "    force_include = force_include or []\n",
    "    force_exclude = force_exclude or []\n",
    "\n",
    "    df = pd.read_csv(observed_csv)\n",
    "    df = df.sort_values([\"sample\", \"genus_count\"], ascending=[True, False])\n",
    "    df = df[df[\"total_count\"] > min_reads]\n",
    "    amr_totals = df.groupby(\"AMR Gene Family\")[\"total_count\"].sum()\n",
    "    total_all = amr_totals.sum()\n",
    "    amrs_to_plot = amr_totals[amr_totals >= total_all * AMR_MIN_FRACTION].index.tolist()\n",
    "\n",
    "    if not amrs_to_plot:\n",
    "        print(\"No AMR Gene Families meet the abundance threshold.\")\n",
    "        return\n",
    "\n",
    "    df = df[df[\"AMR Gene Family\"].isin(amrs_to_plot)]\n",
    "    amrs = sorted(df[\"AMR Gene Family\"].unique())\n",
    "    samples = df[\"sample\"].nunique()\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(amrs),\n",
    "        cols=1,\n",
    "        subplot_titles=amrs,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    for i, amr in enumerate(amrs, start=1):\n",
    "        df_amr = preprocess_abundance(\n",
    "            df, amr, min_genus_abundance, force_include, force_exclude\n",
    "        )\n",
    "        genus_colors = get_genus_colors(df_amr[\"genus\"].unique())\n",
    "\n",
    "        legendgroup = f\"group{i}\"  # unique group per subplot\n",
    "        for genus in df_amr[\"genus\"].unique():\n",
    "            genus_data = df_amr[df_amr[\"genus\"] == genus]\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=genus_data[\"sample_label\"],\n",
    "                    y=genus_data[\"relative_genus_count\"],\n",
    "                    name=genus,\n",
    "                    marker_color=genus_colors[genus],\n",
    "                    legendgroup=legendgroup,\n",
    "                    legendgrouptitle=dict(text=amr) if genus == df_amr[\"genus\"].unique()[0] else None,\n",
    "                    showlegend=True,\n",
    "                ),\n",
    "                row=i,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        # Custom legend positioning for each subplot (optional, only needed if separating legends visually)\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                y=1,\n",
    "                yanchor=\"top\",\n",
    "                x=2.5-np.log10(samples),\n",
    "                xanchor=\"left\",\n",
    "                tracegroupgap=500  # adds spacing between legend groups\n",
    "            ),\n",
    "            margin=dict(r=300)  # enough space for long legends\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\",\n",
    "        title=\"Relative Genus Abundance per AMR Gene Family\",\n",
    "        height=800 * len(amrs),\n",
    "        width=1000 * np.log10(samples) if samples > 2 else 500,\n",
    "        plot_bgcolor=\"white\",\n",
    "        yaxis=dict(tickformat=\".0%\"),\n",
    "        showlegend=True,\n",
    "        margin=dict(r=300),\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Relative Abundance\",categoryorder=\"array\",categoryarray=sorted(df_amr[\"sample_label\"].unique()))\n",
    "\n",
    "    fig.show()\n",
    "    # fig.write_html(output_html)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"\"\n",
    "    min_abundance = 0.01\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    plot_stacked_abundance(input_csv, output_html, float(min_abundance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "RESERVED_COLOR = \"rgb(217,217,217)\"\n",
    "\n",
    "def get_genus_colors(all_genera):\n",
    "    \"\"\"Assign consistent, distinguishable colors to each genus.\"\"\"\n",
    "    top_colors = [\n",
    "        \"#D62728\", \"#FF7F0E\", \"#8B4513\", \"#1F77B4\", \"#800080\",\n",
    "        \"#7F7F7F\", \"#2CA02C\", \"#1E90FF\", \"#BA55D3\", \"#BCBD22\"\n",
    "    ]\n",
    "\n",
    "    fallback_palette = (\n",
    "        px.colors.qualitative.Pastel\n",
    "        + px.colors.qualitative.Set3\n",
    "        + px.colors.qualitative.Alphabet\n",
    "        + px.colors.qualitative.Light24\n",
    "        + px.colors.qualitative.Bold\n",
    "    )\n",
    "\n",
    "    color_pool = list(dict.fromkeys(top_colors + fallback_palette))\n",
    "    if RESERVED_COLOR in color_pool:\n",
    "        color_pool.remove(RESERVED_COLOR)\n",
    "\n",
    "    genus_list = [g for g in all_genera if g != \"Others\"]\n",
    "    if len(genus_list) > len(color_pool):\n",
    "        raise ValueError(\n",
    "            f\"Too many genera ({len(genus_list)}) for available color pool.\"\n",
    "        )\n",
    "\n",
    "    genus_colors = {g: color_pool[i] for i, g in enumerate(genus_list)}\n",
    "    genus_colors[\"Others\"] = RESERVED_COLOR\n",
    "    return genus_colors\n",
    "\n",
    "\n",
    "def preprocess_abundance(df, min_genus_abundance, force_include, force_exclude):\n",
    "    \"\"\"Filter and aggregate genus abundance data.\"\"\"\n",
    "    df_amr = df.copy()\n",
    "\n",
    "    # Group low-abundance or excluded genera into 'Others'\n",
    "    low_abundance = df_amr[\n",
    "        ((df_amr[\"relative_genus_count\"] <= min_genus_abundance) & ~df_amr[\"genus\"].isin(force_include))\n",
    "        | df_amr[\"genus\"].isin(force_exclude)\n",
    "    ]\n",
    "    others = (\n",
    "        low_abundance.groupby([\"sample\", \"total_count\"], as_index=False)\n",
    "        .agg({\"relative_genus_count\": \"sum\"})\n",
    "        .assign(genus=\"Others\")\n",
    "    )\n",
    "    others[\"sample_label\"] = others[\"sample\"] + \" (\" + others[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    # Keep only included & high-abundance genera\n",
    "    df_amr = df_amr[~df_amr[\"genus\"].isin(force_exclude)]\n",
    "    df_amr_filtered = df_amr[\n",
    "        (df_amr[\"relative_genus_count\"] > min_genus_abundance)\n",
    "        | df_amr[\"genus\"].isin(force_include)\n",
    "    ]\n",
    "    df_amr_filtered = df_amr_filtered.sort_values([\"sample\", \"AMR Gene Family\", \"genus_count\"], ascending=[True, False, False])\n",
    "    df_amr_filtered[\"sample_label\"] = df_amr_filtered[\"sample\"] + \" (\" + df_amr_filtered[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    return pd.concat([df_amr_filtered, others], ignore_index=True)\n",
    "\n",
    "\n",
    "def plot_stacked_abundance(\n",
    "    observed_csv,\n",
    "    output_html=None,\n",
    "    min_genus_abundance=0.01,\n",
    "    force_include=None,\n",
    "    force_exclude=None,\n",
    "):\n",
    "    \"\"\"Plot a single stacked bar chart of genus abundance for most prevalent AMR per sample.\"\"\"\n",
    "    force_include = force_include or []\n",
    "    force_exclude = force_exclude or []\n",
    "\n",
    "    df = pd.read_csv(observed_csv)\n",
    "    amr = df.groupby(\"AMR Gene Family\")[\"total_count\"].sum().idxmax()\n",
    "    print(amr)\n",
    "    # Keep only AMR with highest total_count per sample\n",
    "    df_max = df[df[\"AMR Gene Family\"] == amr]\n",
    "    display(df_max)\n",
    "    amrs = df_max[\"AMR Gene Family\"].unique()\n",
    "    samples = df_max[\"sample\"].nunique()\n",
    "\n",
    "    df_amr = preprocess_abundance(df_max, min_genus_abundance, force_include, force_exclude)\n",
    "    genus_colors = get_genus_colors(df_amr[\"genus\"].unique())\n",
    "\n",
    "    fig = go.Figure()\n",
    "    legend_added = set()\n",
    "\n",
    "    for genus in df_amr[\"genus\"].unique():\n",
    "        genus_data = df_amr[df_amr[\"genus\"] == genus]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=genus_data[\"sample_label\"],\n",
    "                y=genus_data[\"relative_genus_count\"],\n",
    "                name=genus,\n",
    "                marker_color=genus_colors[genus],\n",
    "                showlegend=genus not in legend_added,\n",
    "            )\n",
    "        )\n",
    "        legend_added.add(genus)\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\",\n",
    "        title=f\"Relative Genus Abundance (Most Abundant AMR Gene Family: {', '.join(amrs)})\",\n",
    "        height=800,\n",
    "        width=1000 * np.log10(samples) if samples > 2 else 500,\n",
    "        plot_bgcolor=\"white\",\n",
    "        legend_title=\"Genus\",\n",
    "        xaxis=dict(tickangle=45),\n",
    "        yaxis=dict(title=\"Relative Abundance\", tickformat=\".0%\"),\n",
    "    )\n",
    "\n",
    "    if output_html:\n",
    "        fig.write_html(output_html)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"\"\n",
    "    min_abundance = 0.01\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    plot_stacked_abundance(input_csv, output_html, float(min_abundance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create bubble plot\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: abundance file\n",
    "# Output: bubble plot per sample\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "bubble_plot = result_dir / \"combined_genus_abundance_bubbleplot.html\"\n",
    "\n",
    "# === Bubble Plot Script ===\n",
    "def load_filtered_data(input_csv, min_total_count=100):\n",
    "    \"\"\"Load CSV and filter AMR Gene Families by minimum total genus count\"\"\"\n",
    "    df = pd.read_csv(input_csv, sep=\",\")\n",
    "    return df[df[\"total_count\"] > min_total_count]\n",
    "\n",
    "\n",
    "def get_top_genera_per_sample(df, top_n):\n",
    "    \"\"\"Return dicts of top genera per sample (set and list forms)\"\"\"\n",
    "    top_sets = {}\n",
    "    top_lists = {}\n",
    "    for sample in df[\"sample\"].unique():\n",
    "        sample_df = df[df[\"sample\"] == sample]\n",
    "        top = (\n",
    "            sample_df.sort_values(by=\"relative_genus_count\", ascending=False)\n",
    "            .head(top_n)[\"genus\"]\n",
    "            .tolist()\n",
    "        )\n",
    "        top_sets[sample] = set(top)\n",
    "        top_lists[sample] = top\n",
    "    return top_sets, top_lists\n",
    "\n",
    "\n",
    "def select_genera(top_sets, top_lists, max_genera, min_overlap):\n",
    "    \"\"\"Select a list of genera to display using overlap or merged ranking\"\"\"\n",
    "    if not top_sets:\n",
    "        return []\n",
    "\n",
    "    overlap = set.intersection(*top_sets.values())\n",
    "    total_genus = sum(len(lst) for lst in top_lists.values())\n",
    "\n",
    "    if len(overlap) >= min_overlap:\n",
    "        return list(overlap)[:max_genera]\n",
    "    elif total_genus > max_genera:\n",
    "        combined = set(overlap)\n",
    "        sample_iters = {s: iter(l) for s, l in top_lists.items()}\n",
    "\n",
    "        while len(combined) < max_genera:\n",
    "            for gen_iter in sample_iters.values():\n",
    "                try:\n",
    "                    while True:\n",
    "                        genus = next(gen_iter)\n",
    "                        if genus not in combined:\n",
    "                            combined.add(genus)\n",
    "                            break\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                if len(combined) >= max_genera:\n",
    "                    break\n",
    "        return list(combined)\n",
    "    else:\n",
    "        return list({genus for sublist in top_lists.values() for genus in sublist})\n",
    "\n",
    "\n",
    "def add_amr_family_subplot(\n",
    "    fig, df, amr_family, col_idx, max_genera, min_overlap, top_per_sample\n",
    "):\n",
    "    \"\"\"Filter and add a subplot for one AMR Gene Family to the main figure\"\"\"\n",
    "    df_amr = df[df[\"AMR Gene Family\"] == amr_family]\n",
    "    if df_amr.empty:\n",
    "        return\n",
    "\n",
    "    top_sets, top_lists = get_top_genera_per_sample(df_amr, top_per_sample)\n",
    "    selected = select_genera(top_sets, top_lists, max_genera, min_overlap)\n",
    "    df_plot = df_amr[df_amr[\"genus\"].isin(selected)]\n",
    "\n",
    "    scatter = px.scatter(\n",
    "        df_plot,\n",
    "        x=\"sample\",\n",
    "        y=\"genus\",\n",
    "        size=\"relative_genus_count\",\n",
    "        color=\"total_count\",\n",
    "        hover_name=\"genus\",\n",
    "        hover_data={\n",
    "            \"genus_count\": True,\n",
    "            \"relative_genus_count\": True,\n",
    "            \"total_count\": True,\n",
    "            \"sample\": False,\n",
    "        },\n",
    "        size_max=20,\n",
    "        color_continuous_scale=\"Greens\",\n",
    "    )\n",
    "\n",
    "    for trace in scatter.data:\n",
    "        fig.add_trace(trace, row=1, col=col_idx)\n",
    "\n",
    "\n",
    "def create_bubble_plot_grid(df, max_genera, min_overlap, top_per_sample):\n",
    "    \"\"\"Create the full multi-subplot bubble chart\"\"\"\n",
    "    families = df[\"AMR Gene Family\"].unique()\n",
    "    num_cols = len(families) if len(df) > 1 else 1\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=num_cols,\n",
    "        subplot_titles=list(families),\n",
    "        horizontal_spacing=0.2,\n",
    "    )\n",
    "\n",
    "    for idx, family in enumerate(families, start=1):\n",
    "        add_amr_family_subplot(\n",
    "            fig, df, family, idx, max_genera, min_overlap, top_per_sample\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Bubble Plots of Top Genera for Each AMR Gene Family\",\n",
    "        plot_bgcolor=\"lightgrey\",\n",
    "        height=900,\n",
    "        width=500 * num_cols,\n",
    "        coloraxis_colorbar=dict(title=\"Fusion Read Count\"),\n",
    "    )\n",
    "    fig.update_yaxes(categoryorder=\"category descending\")\n",
    "    fig.update_xaxes(categoryorder=\"category ascending\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_bubble_plots_combined(\n",
    "    input_csv, output_html, max_genera=20, min_overlap=10, top_per_sample=20\n",
    "):\n",
    "    \"\"\"Load input, pass to processing function and save plot\"\"\"\n",
    "    df = load_filtered_data(input_csv)\n",
    "    fig = create_bubble_plot_grid(df, max_genera, min_overlap, top_per_sample)\n",
    "    pio.write_html(fig, file=output_html)\n",
    "    pio.show(fig)\n",
    "\n",
    "create_bubble_plots_combined(abundance_result, bubble_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create boxplots\n",
    "# Selfwritten python script \"percidt_per_genus.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: boxplot over all samples per percentage identity, number of unique hits and genera\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "boxplot = result_dir / \"genus_idt_per_genus_plot.png\"\n",
    "\n",
    "\n",
    "def generate_percentage_idt_per_genus(input_files, output_file):\n",
    "    all_data = []  # List to hold DataFrames from all input files\n",
    "\n",
    "    for input_file in [str(input_files)]:\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            sep=\",\",\n",
    "            header=0,\n",
    "        )\n",
    "        all_data.append(df)\n",
    "\n",
    "    # Combine all partitions into a single DataFrame\n",
    "    combined_data = pd.concat(all_data)\n",
    "\n",
    "    # Calculate genus query counts\n",
    "    genus_query_counts = (\n",
    "        combined_data.groupby(\"genus\")[\"query_id\"].nunique().reset_index()\n",
    "    )\n",
    "    genus_query_counts.columns = [\"genus\", \"unique_query_count\"]\n",
    "\n",
    "    # Keep only the top 20 genera\n",
    "    top20_species = genus_query_counts.nlargest(20, \"unique_query_count\")\n",
    "\n",
    "    # Filter combined_data to retain only the top 20 genera\n",
    "    combined_data = combined_data[combined_data[\"genus\"].isin(top20_species[\"genus\"])]\n",
    "\n",
    "    # Now filter genus_query_counts as well\n",
    "    genus_query_counts = genus_query_counts[\n",
    "        genus_query_counts[\"genus\"].isin(top20_species[\"genus\"])\n",
    "    ]\n",
    "\n",
    "    # Define order for the x-axis\n",
    "    genus_order = top20_species.sort_values(by=\"unique_query_count\", ascending=False)[\n",
    "        \"genus\"\n",
    "    ]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    sns.boxplot(\n",
    "        x=\"genus\",\n",
    "        y=\"perc_identity_16S\",\n",
    "        data=combined_data,\n",
    "        ax=ax1,\n",
    "        order=genus_order,\n",
    "        fliersize=0.0,\n",
    "        color=\"dodgerblue\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Bacterial Genus\")\n",
    "    ax1.set_ylabel(\"Percentage Identity (boxplot)\", color=\"royalblue\")\n",
    "    ax1.set_title(\n",
    "        \"Boxplot of Percentage Identity and Read Counts for Each Bacterial Genus\"\n",
    "    )\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Add a second y-axis for unique query counts\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.barplot(\n",
    "        x=\"genus\",\n",
    "        y=\"unique_query_count\",\n",
    "        data=genus_query_counts,\n",
    "        ax=ax2,\n",
    "        alpha=0.2,\n",
    "        color=\"purple\",\n",
    "        order=genus_order,\n",
    "    )\n",
    "    ax2.set_ylabel(\"Number of hits (bar)\", color=\"violet\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "generate_percentage_idt_per_genus(filter_result, boxplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create boxplots\n",
    "# Selfwritten python scripts \"boxplot_[align_lengths,evalue,percidt].py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: boxplot over all samples per parameter alignment lengths, E-value or percentage identity\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "This script takes a list of all filtered fasta files, combines e-value information \n",
    "across samples, and visualizes the distribution of e-values using boxplots split \n",
    "by part (ABR/16S) and sample.\n",
    "\"\"\"\n",
    "\n",
    "PRETTY_LABELS = {\n",
    "    \"align_length\": \"Alignment length\",\n",
    "    \"perc_identity\": \"Percentage identity\",\n",
    "    \"evalue\": \"E-value\"\n",
    "}\n",
    "\n",
    "def read_and_process_partitioned_data(partition_files, sample, param):\n",
    "    \"\"\"Read and process partitioned files for a single sample.\"\"\"\n",
    "    data_frames = []\n",
    "    sample_name = sample\n",
    "    param = param\n",
    "    for part_file in partition_files:\n",
    "        if os.path.exists(part_file):\n",
    "            df = pd.read_csv(\n",
    "                part_file, header=0, sep=\",\"\n",
    "            )\n",
    "            #df[f\"{param}_ABR\"] = df[f\"{param}_ABR\"] * 3\n",
    "            long_df = pd.melt(\n",
    "                df,\n",
    "                id_vars=[\"query_id\"],\n",
    "                value_vars=[param + \"_ABR\", param + \"_16S\"],\n",
    "                var_name=\"part\",\n",
    "                value_name=param\n",
    "            )\n",
    "\n",
    "            # Normalize part labels\n",
    "            long_df[\"part\"] = long_df[\"part\"].str.replace(param + \"_\", \"\")\n",
    "            long_df[\"sample\"] = sample_name\n",
    "            data_frames.append(long_df)\n",
    "        \n",
    "    if data_frames:\n",
    "        return pd.concat(data_frames)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def plot_boxplots(data, output_file):\n",
    "    \"\"\"\n",
    "    Generate and save boxplots of e-values across samples and parts (ABR vs. 16S).\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Combined dataframe containing 'sample', 'evalue', and 'part'.\n",
    "        output_file (str): Path to save the resulting plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    flierprops = dict(markerfacecolor=\"0.75\", markersize=2, linestyle=\"none\")\n",
    "    sns.boxplot(x=\"sample\", y=\"perc_identity\", hue=\"part\", data=data, flierprops=flierprops)\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.title(\"Boxplot of e-values for ABR and 16S parts across samples -Filtered-\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Percentage identity\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(filtered_fasta_files, sample_names, param, output_file):\n",
    "    \"\"\"Main function to process partitioned files for each sample and generate the plot.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Loop over each sample's partitioned CSV files\n",
    "    for sample in sample_names:\n",
    "        data = read_and_process_partitioned_data(\n",
    "            [file for file in filtered_fasta_files], sample, param\n",
    "        )\n",
    "        if data is not None:\n",
    "            all_data.append(data)\n",
    "\n",
    "    if all_data:\n",
    "        combined_data = pd.concat(all_data)\n",
    "        plot_boxplots(combined_data, output_file)\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "    result_dir = base / \".test_steps/results\"\n",
    "\n",
    "    filter_result = result_dir / \"filtered_result.csv\"\n",
    "    boxplot = result_dir / f\"combine_boxplot.png\"\n",
    "    \n",
    "    filtered_fasta_files = filter_result\n",
    "    output_file = boxplot  # Single output file for all panels\n",
    "    sample_names = \"test_epic_data\"\n",
    "    param = \"perc_identity\"\n",
    "    main([str(filtered_fasta_files)], [sample_names], param, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create Attrition plot\n",
    "# Selfwritten python scripts \"plot_attrition.py\"\n",
    "# Input: overview table\n",
    "# Output: plot of count overview throughout ERMA process with respect to rejection breakdown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "sample = \"test_epic_data\"\n",
    "result_dir = base / \".test_steps/results\"\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "overview_plot = result_dir / \"overview_plot.png\"\n",
    "\n",
    "# === Category Definitions (now match the final labels directly) ===\n",
    "MAIN_CATEGORIES = [\n",
    "    \"Number of FastQ input reads\",\n",
    "    \"Merged similarity hits\",\n",
    "    \"Filtered fusion reads\",\n",
    "]\n",
    "\n",
    "FILTER_REASONS = {\n",
    "    \"Diamond hits < similarity threshold\": \"royalblue\",\n",
    "    \"Diamond hits NOT highest percentage identity per query\": \"purple\",\n",
    "    \"Usearch hits < similarity threshold\": \"#a6d854\",\n",
    "    \"Usearch hits NOT highest percentage identity per query\": \"#66c2a5\",\n",
    "    \"Query hit in only one of two databases\": \"#ffd92f\",\n",
    "}\n",
    "\n",
    "MAIN_COLOR_MAP = {\n",
    "    \"Number of FastQ input reads\": \"seagreen\",\n",
    "    \"Merged similarity hits\": \"#fc8d62\",\n",
    "    \"Filtered fusion reads\": \"#8da0cb\",\n",
    "}\n",
    "\n",
    "# === Load and summarize the table ===\n",
    "def load_and_summarize_data(path):\n",
    "    df = pd.read_csv(path, names=[\"sample\", \"state\", \"count\"])\n",
    "    df[\"count\"] = df[\"count\"].astype(int).abs()\n",
    "\n",
    "    main_df = df[df[\"state\"].isin(MAIN_CATEGORIES)].pivot(index=\"sample\", columns=\"state\", values=\"count\").fillna(0)\n",
    "    filter_df = df[df[\"state\"].isin(FILTER_REASONS)].pivot(index=\"sample\", columns=\"state\", values=\"count\").fillna(0)\n",
    "\n",
    "    return main_df, filter_df\n",
    "\n",
    "# === Plotting function ===\n",
    "def plot_summary(main_df, filter_df, output_path):\n",
    "    samples = main_df.index\n",
    "    x = np.arange(len(samples))\n",
    "    bar_width = 0.18\n",
    "    overlay_width = 0.1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Plot main bars with offsets\n",
    "    offsets = np.linspace(-bar_width, bar_width, len(MAIN_CATEGORIES))\n",
    "    for i, col in enumerate(MAIN_CATEGORIES):\n",
    "        if col not in main_df.columns:\n",
    "            continue\n",
    "        ax.bar(\n",
    "            x + offsets[i],\n",
    "            main_df[col],\n",
    "            bar_width,\n",
    "            label=col,\n",
    "            color=MAIN_COLOR_MAP.get(col, \"gray\"),\n",
    "        )\n",
    "\n",
    "    # Plot filter stack bars *on top* of \"Filtered fusion reads\"\n",
    "    if \"Filtered fusion reads\" in main_df.columns:\n",
    "        bottom = main_df[\"Filtered fusion reads\"].values.copy()\n",
    "    else:\n",
    "        bottom = np.zeros_like(x)\n",
    "\n",
    "    for reason in FILTER_REASONS:\n",
    "        heights = filter_df[reason].values if reason in filter_df.columns else np.zeros_like(x)\n",
    "        ax.bar(\n",
    "            x + bar_width,\n",
    "            heights,\n",
    "            overlay_width,\n",
    "            bottom=bottom,\n",
    "            label=reason,\n",
    "            color=FILTER_REASONS.get(reason, \"gray\"),\n",
    "        )\n",
    "        bottom += heights\n",
    "\n",
    "    # Axis formatting\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(samples, rotation=45)\n",
    "    ax.set_ylabel(\"Similarity search hit count\")\n",
    "    ax.set_xlabel(\"Sample\")\n",
    "    ax.set_title(\"Similarity Search Processing with Rejection Breakdown\")\n",
    "\n",
    "    # Split legend into main vs. filter\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    main_labels = MAIN_CATEGORIES\n",
    "    filter_labels = FILTER_REASONS\n",
    "\n",
    "    legend1 = ax.legend(\n",
    "        [handles[labels.index(l)] for l in main_labels if l in labels],\n",
    "        main_labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        title=\"Hit Process\",\n",
    "    )\n",
    "    legend2 = ax.legend(\n",
    "        [handles[labels.index(l)] for l in filter_labels if l in labels],\n",
    "        filter_labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.02, 0.55),\n",
    "        title=\"Filtering Reasons\",\n",
    "    )\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "# === Execute ===\n",
    "main_df, filter_df = load_and_summarize_data(overview_table)\n",
    "plot_summary(main_df, filter_df, overview_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create Abundance data\n",
    "# Selfwritten python script \"single_genera_abundance_table.py\"\n",
    "# Input: Overview table created iteritavely within the snakemake run\n",
    "# Output: barplots for all samples showing generated and filtered similarity search hits\n",
    "# Note: Overview Table is created here after the process while in the original snakemake run\n",
    "#       it's created iteratively within the workflow.\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def write_dummy_line(sample_name):\n",
    "    dummy_line = {\n",
    "        \"sample\": sample_name,\n",
    "        \"AMR Gene Family\": \"NA\",\n",
    "        \"genus\": \"NA\",\n",
    "        \"genus_count\": 0,\n",
    "        \"total_count\": 0,\n",
    "        \"relative_genus_count\": 0,\n",
    "    }\n",
    "    merged_data = pd.DataFrame([dummy_line])\n",
    "    return merged_data\n",
    "\n",
    "def process_combined_data(combined_data, sample_name):\n",
    "\n",
    "    combined_data[\"sample\"] = sample_name\n",
    "\n",
    "    genus_counts = (\n",
    "        combined_data.groupby([\"sample\", \"AMR Gene Family\", \"genus\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"genus_count\")\n",
    "    )\n",
    "\n",
    "    total_counts = (\n",
    "        genus_counts.groupby([\"sample\", \"AMR Gene Family\"])[\"genus_count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_count\")\n",
    "    )\n",
    "\n",
    "    genus_counts = pd.merge(\n",
    "        genus_counts, total_counts, on=[\"sample\", \"AMR Gene Family\"], how=\"left\"\n",
    "    )\n",
    "    genus_counts[\"relative_genus_count\"] = round(\n",
    "        genus_counts[\"genus_count\"] / genus_counts[\"total_count\"], 4\n",
    "    )\n",
    "\n",
    "    return genus_counts\n",
    "\n",
    "\n",
    "def export_genera_abundance(input_files, sample_name, parts, output_path):\n",
    "    sample_input_files = [f for f in input_files]\n",
    "    part_dfs = []\n",
    "    for part in parts:\n",
    "        matching_files = [f for f in sample_input_files]\n",
    "        print(sample_input_files,matching_files)\n",
    "        if not matching_files:\n",
    "            continue\n",
    "        input_file = matching_files[0]\n",
    "        df = pd.read_csv(\n",
    "            input_file, sep=\",\",  header=0\n",
    "        )\n",
    "        part_dfs.append(df)\n",
    "\n",
    "    if not part_dfs:\n",
    "        print(f\"No valid parts found for sample: {sample_name}\")\n",
    "        dummy_df = write_dummy_line(sample_name)\n",
    "        dummy_df.to_csv(output_path, index=False)\n",
    "        return        \n",
    "\n",
    "    full_sample_df = pd.concat(part_dfs, ignore_index=True)\n",
    "    processed_data = process_combined_data(full_sample_df, sample_name)\n",
    "\n",
    "    processed_data = processed_data.sort_values(\n",
    "        by=[\"sample\", \"genus_count\"], ascending=False\n",
    "    )\n",
    "\n",
    "    display(processed_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "    result_dir = base / \".test_steps/results\"\n",
    "\n",
    "    filter_result = result_dir / \"filtered_result.csv\"\n",
    "    table = result_dir / f\"single_abundance_table.csv\"\n",
    "    \n",
    "    filtered_fasta_files = filter_result\n",
    "    \n",
    "    input_file = filter_result\n",
    "    output_path = table\n",
    "    sample_name = \"test_epic_data\"    \n",
    "    parts = [\"001\"]\n",
    "    export_genera_abundance([str(input_file)], sample_name, parts, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path().resolve()\n",
    "result_dir = base / \"results\"\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "overview_html = \"overview_table.html\"\n",
    "\n",
    "# Read the input table\n",
    "df = pd.read_csv(overview_table, sep=\",\", header=None, names=[\"sample\",\"step\",\"total_count\"])\n",
    "\n",
    "# Mapping step -> State\n",
    "step_to_state = {\n",
    "    \"Number of FastQ input reads\": \"Input reads\",\n",
    "    \"Diamond output hits\": \"Similarity search\",\n",
    "    \"Usearch output hits\": \"Similarity search\",\n",
    "    \"Merged similarity hits\": \"Similarity search\",\n",
    "    \"Diamond hits < similarity threshold\": \"Filtration\",\n",
    "    \"Diamond hits NOT highest percentage identity per query\": \"Filtration\",\n",
    "    \"Usearch hits < similarity threshold\": \"Filtration\",\n",
    "    \"Usearch hits NOT highest percentage identity per query\": \"Filtration\",\n",
    "    \"Query hit in only one of two databases\": \"Filtration\",\n",
    "    \"Filtered fusion reads\": \"Output reads\"\n",
    "}\n",
    "\n",
    "df[\"state\"] = df[\"step\"].map(step_to_state)\n",
    "\n",
    "# Reorder and sort\n",
    "df = df[[\"sample\", \"state\", \"step\", \"total_count\"]]\n",
    "state_order = [\"Input reads\", \"Similarity search\", \"Filtration\", \"Output reads\"]\n",
    "df[\"state\"] = pd.Categorical(df[\"state\"], categories=state_order, ordered=True)\n",
    "df = df.sort_values(by=[\"sample\", \"state\"])\n",
    "\n",
    "# === HTML with rowspan for merged cells ===\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table.styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 0.95em;\n",
    "        font-family: sans-serif;\n",
    "        min-width: 600px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    table.styled-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    table.styled-table th,\n",
    "    table.styled-table td {\n",
    "        padding: 10px 12px;\n",
    "        border: 1px solid #ddd;\n",
    "    }\n",
    "    table.styled-table tbody tr:nth-child(even) {\n",
    "        background-color: #f3f3f3;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table class=\"styled-table\">\n",
    "<thead>\n",
    "    <tr><th>Sample</th><th>State</th><th>Step</th><th>Count</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Group and track rowspans\n",
    "grouped = df.groupby([\"sample\", \"state\"])\n",
    "for (sample, state), group in grouped:\n",
    "    sample_rowspan = len(df[df[\"sample\"] == sample])\n",
    "    state_rowspan = len(group)\n",
    "    \n",
    "    first_state = True\n",
    "    for i, row in group.iterrows():\n",
    "        html += \"<tr>\"\n",
    "        if i == df[df[\"sample\"] == sample].index[0]:\n",
    "            html += f'<td rowspan=\"{sample_rowspan}\">{sample}</td>'\n",
    "        if first_state:\n",
    "            html += f'<td rowspan=\"{state_rowspan}\">{state}</td>'\n",
    "            first_state = False\n",
    "        html += f\"<td>{row['step']}</td><td>{row['total_count']}</td>\"\n",
    "        html += \"</tr>\"\n",
    "\n",
    "html += \"\"\"\n",
    "</tbody>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(html))\n",
    "# Write to file\n",
    "with open(overview_html, \"w\") as f:\n",
    "    f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path().resolve()\n",
    "result_dir = base / \"results\"\n",
    "overview_table = result_dir / \"genera_abundance.csv\"\n",
    "overview_html = \"\"\n",
    "\n",
    "# Read the input table\n",
    "df = pd.read_csv(overview_table, sep=\",\", header=0)\n",
    "\n",
    "# === HTML with rowspan for merged cells ===\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table.styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 0.95em;\n",
    "        font-family: sans-serif;\n",
    "        min-width: 600px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    table.styled-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    table.styled-table th,\n",
    "    table.styled-table td {\n",
    "        padding: 10px 12px;\n",
    "        border: 1px solid #ddd;\n",
    "    }\n",
    "    table.styled-table tbody tr:nth-child(even) {\n",
    "        background-color: #f3f3f3;\n",
    "    }\n",
    "    table.styled-table tbody tr:hover {\n",
    "        background-color: #f1f1f1;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table class=\"styled-table\">\n",
    "<thead>\n",
    "    <tr><th>Sample</th><th>AMR Gene Family</th><th>Genus</th><th>Fusion Read Count</th><th>Relative</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Group and track rowspans\n",
    "grouped = df.groupby([\"sample\", \"AMR Gene Family\"])\n",
    "for (sample, family), group in grouped:\n",
    "    sample_rowspan = len(df[df[\"sample\"] == sample])\n",
    "    family_rowspan = len(group)\n",
    "    amr = df[(df[\"sample\"] == sample) & (df[\"AMR Gene Family\"] == family)]\n",
    "    reads_per_amr = amr[\"genus_count\"].sum()\n",
    "    amr_line = f\"{family}<br><span style='font-size: 0.85em'> Total Fusion Reads: {reads_per_amr}</span>\"\n",
    "    first_family = True\n",
    "    for i, row in group.iterrows():\n",
    "        html += \"<tr>\"\n",
    "        if i == df[df[\"sample\"] == sample].index[0]:\n",
    "            html += f'<td rowspan=\"{sample_rowspan}\">{sample}</td>'\n",
    "        if first_family:\n",
    "            html += f'<td rowspan=\"{family_rowspan}\">{amr_line}</td>'\n",
    "            first_family = False\n",
    "        html += f\"<td>{row['genus']}</td><td>{row['genus_count']}</td><td>{row['relative_genus_count']}</td>\"\n",
    "        html += \"</tr>\"\n",
    "\n",
    "html += \"\"\"\n",
    "</tbody>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(html))\n",
    "# Write to file\n",
    "#with open(overview_html, \"w\") as f:\n",
    "#    f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Root log directory (adjust if needed)\n",
    "log_dir = \"/local/work/adrian/ERMA/logs\"   # replace with your actual path\n",
    "output = \"/local/work/adrian/ERMA/logs/logs.json\"\n",
    "\n",
    "logs = {}\n",
    "\n",
    "for rule in sorted(os.listdir(log_dir)):\n",
    "    rule_path = os.path.join(log_dir, rule)\n",
    "    if not os.path.isdir(rule_path):\n",
    "        continue\n",
    "\n",
    "    rule_logs = {}\n",
    "    for log_file in sorted(os.listdir(rule_path)):\n",
    "        file_path = os.path.join(rule_path, log_file)\n",
    "\n",
    "        # Skip empty files\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "\n",
    "        # Use filename without extension as sample name\n",
    "        sample = os.path.splitext(log_file)[0]\n",
    "\n",
    "        # Read log text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        rule_logs[sample] = text\n",
    "\n",
    "    if rule_logs:  # only keep non-empty rules\n",
    "        logs[rule] = rule_logs\n",
    "\n",
    "# Write JSON\n",
    "with open(output, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(logs, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"logs.json written with\", sum(len(v) for v in logs.values()), \"log entries.\")\n",
    "\n",
    "# Remove all subfolders in the log directory\n",
    "for rule in os.listdir(log_dir):\n",
    "    rule_path = os.path.join(log_dir, rule)\n",
    "    if os.path.isdir(rule_path):\n",
    "        shutil.rmtree(rule_path)\n",
    "\n",
    "print(\"All subfolders removed, only logs.json remains.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
