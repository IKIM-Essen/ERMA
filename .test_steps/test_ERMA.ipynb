{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src= \"ERMA_workflow.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "card_fasta = \"/local/work/adrian/ERMA/data/card_db/protein_fasta_protein_homolog_model.fasta\"\n",
    "output_fasta = \"/local/work/adrian/mge_databases/test_ERMA_carddb/protein_fasta_with_uniprot.fasta\"\n",
    "targets = [\"int1\",\"inti1\"]\n",
    "cluster = \"100\"\n",
    "size = 500\n",
    "log = \"/local/work/adrian/mge_databases/test_ERMA_carddb/log.txt\"\n",
    "\n",
    "base_url = \"https://rest.uniprot.org/uniref/search\"\n",
    "headers = {\"accept\": \"text/plain\"}\n",
    "identity_map = {\"100\": \"1.0\", \"90\": \"0.9\", \"50\": \"0.5\"}\n",
    "\n",
    "\n",
    "def fetch_all(base_url, params, headers):\n",
    "    \"\"\"Fetch all pages from UniProt REST API.\"\"\"\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "with open(log, \"w\") as lf, open(output_fasta, \"w\") as out:\n",
    "    lf.write(f\"Extending CARD with UniRef{cluster} sequences for targets: {targets}\\n\")\n",
    "\n",
    "    # Write original CARD sequences first\n",
    "    with open(card_fasta, \"r\") as cf:\n",
    "        out.write(cf.read())\n",
    "\n",
    "    for t in targets:\n",
    "        query = f\"{t} AND identity:{identity_map[cluster]}\"\n",
    "        params = {\"query\": query, \"format\": \"fasta\", \"size\": size}\n",
    "        lf.write(f\"Fetching UniRef{cluster} sequences for: {t}\\n\")\n",
    "        try:\n",
    "            fasta = fetch_all(base_url, params, headers)\n",
    "            out.write(fasta)\n",
    "        except Exception as e:\n",
    "            lf.write(f\"Failed to fetch {t}: {e}\\n\")\n",
    "        time.sleep(0.5)  # UniProt’s API enforces ~3 requests/sec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Step similarity search\n",
    "# Input: fasta files\n",
    "# Output: tabular similarity search results\n",
    "# commands:\n",
    "# usearch vs silva_v138.2 database (510495 reads): usearch -usearch_local {input.fasta} -db {input.silva} -blast6out {output.silva_results} -evalue 1e-5 -threads {params.internal_threads} -strand plus -mincols 200 2> {log}\n",
    "# diamond vs card_v3.3.0 database (4840 reads): diamond blastx -d {input.card} -q {input.fasta} -o {output.card_results} --outfmt 6 --evalue 1e-5 --quiet --threads {params.internal_threads} 2> {log}\n",
    "# Notes: Many rules that prepare the similarity search are reproduced with simple bash commands\n",
    "\n",
    "import subprocess\n",
    "import pathlib, os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from IPython.display import display, Markdown\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "github = base / \".github\"\n",
    "\n",
    "silva_dir = github / \"data/silva_db\"\n",
    "card_dir = github / \"data/card_db\"\n",
    "fastq_dir = github / \"data/fastq\"\n",
    "test_out = base / \".test_steps\"\n",
    "\n",
    "targets = [\"int1\",\"inti1\",\"class_1_integron\"]\n",
    "cluster = \"100\"\n",
    "size = 1000\n",
    "\n",
    "base_url = \"https://rest.uniprot.org/uniref/search\"\n",
    "headers = {\"accept\": \"text/plain\"}\n",
    "identity_map = {\"100\": \"1.0\", \"90\": \"0.9\", \"50\": \"0.5\"}\n",
    "\n",
    "fastq = fastq_dir / \"test_epic_data.fastq.gz\"\n",
    "fasta = fastq.with_suffix(\".fasta\")\n",
    "silva_gz = silva_dir / \"sub_silva_seq_RNA.fasta.gz\"\n",
    "silva_fa = silva_gz.with_suffix(\"\")\n",
    "translated_silva = silva_fa.with_name(silva_fa.name.replace(\"_RNA\", \"\"))\n",
    "card_tar = card_dir / \"card_seq.tar.bz2\"\n",
    "card_fasta = card_dir / \"protein_fasta_protein_homolog_model.fasta\"\n",
    "output_fasta = card_dir / \"protein_fasta_with_uniprot.fasta\"\n",
    "output_norm = card_dir / \"protein_fasta_normalized.fasta\"\n",
    "card_db = card_dir / \"card_db.dmnd\"\n",
    "result_dir = test_out / \"results\"\n",
    "card_results = result_dir / \"card_results.txt\"\n",
    "silva_results = result_dir / \"SILVA_results.txt\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd, silent=False):\n",
    "    result = subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL if silent else None, stderr=subprocess.DEVNULL if silent else None)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "\n",
    "def count_lines(file, pattern=None):\n",
    "    cmd = f\"grep -c '{pattern}' {file}\" if pattern else f\"wc -l < {file}\"\n",
    "    return int(subprocess.check_output(cmd, shell=True))\n",
    "\n",
    "def clean(folder, keep):\n",
    "    for item in Path(folder).iterdir():\n",
    "        if item.name not in keep:\n",
    "            if item.is_file():\n",
    "                item.unlink()\n",
    "            elif item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "\n",
    "def fetch_all(base_url, params, headers, max_entries=1000):\n",
    "    \"\"\"Fetch pages from UniProt REST API with an optional total entry limit.\"\"\"\n",
    "    all_data = []\n",
    "    total_entries = 0\n",
    "\n",
    "    while True:\n",
    "        resp = requests.get(base_url, headers=headers, params=params)\n",
    "        resp.raise_for_status()\n",
    "        text = resp.text\n",
    "\n",
    "        # Count FASTA entries\n",
    "        n_entries = text.count(\">\")\n",
    "        total_entries += n_entries\n",
    "        all_data.append(text)\n",
    "\n",
    "        # Stop if limit reached\n",
    "        if max_entries and total_entries >= max_entries:\n",
    "            print(f\"Reached max_entries={max_entries}, stopping.\")\n",
    "            break\n",
    "\n",
    "        # Check for next page\n",
    "        next_link = resp.links.get(\"next\", {}).get(\"url\")\n",
    "        if not next_link:\n",
    "            break\n",
    "        base_url = next_link\n",
    "        params = {}\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return \"\".join(all_data)\n",
    "\n",
    "# === Prepare and run similarity search ===\n",
    "run(f\"mkdir -p {result_dir}\")\n",
    "run(f\"seqtk seq -a {fastq} > {fasta}\")\n",
    "try:\n",
    "    os.remove(silva_fa)\n",
    "except OSError:\n",
    "    pass\n",
    "run(f\"gzip -dk {silva_gz}\")\n",
    "run(f\"seqtk seq -r {silva_fa} > {translated_silva}\")\n",
    "run(f\"tar -xjf {card_tar} -C {card_dir}\")\n",
    "\n",
    "with open(output_fasta, \"w\") as out:\n",
    "    with open(card_fasta, \"r\") as cf:\n",
    "        out.write(cf.read())\n",
    "\n",
    "    for t in targets:\n",
    "        query = f\"{t} AND identity:{identity_map[cluster]}\"\n",
    "        params = {\"query\": query, \"format\": \"fasta\"}\n",
    "        fasta_fetch = fetch_all(base_url, params, headers,size)\n",
    "        out.write(fasta_fetch)\n",
    "        time.sleep(0.5)  # UniProt’s API enforces ~3 requests/sec.\n",
    "\n",
    "\n",
    "run(f\"seqkit rmdup -s {output_fasta} > {'temp.fa'}\")\n",
    "run(f\"seqtk seq -l 0 {'temp.fa'} > {output_norm}\")\n",
    "run(f\"diamond makedb --in {output_norm} -d {card_db.with_suffix('')}\")\n",
    "run(f'diamond blastx -d {card_db} -q {fasta} -o {card_results} --outfmt 6 --evalue 1e-5 --threads 1 --quiet')\n",
    "run(f\"usearch -usearch_local {fasta} -db {translated_silva} -blast6out {silva_results} -evalue 1e-5 -threads 1 -strand plus -mincols 200 > /dev/null 2>&1\", silent=True)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\nsample,state,total_count\")\n",
    "print(f\"Number of FastQ input reads,{count_lines(fasta, '^>')}\")\n",
    "print(f\"Diamond output hits,test,{count_lines(card_results)}\")\n",
    "print(f\"Usearch output hits,test,{count_lines(silva_results)}\")\n",
    "\n",
    "# === Cleanup ===\n",
    "#clean(card_dir, {card_tar.name})\n",
    "for f in fastq_dir.glob(\"*.fasta\"): f.unlink()\n",
    "for f in silva_dir.glob(\"*.fasta\"): f.unlink()\n",
    "\n",
    "# === Report ===\n",
    "display(Markdown(f\"### Processing Complete\\n- CARD hits: `{count_lines(card_results)}`\\n- SILVA hits: `{count_lines(silva_results)}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Integrate similarity search results\n",
    "# Selfwritten python script \"integrate_blast_data.py\"\n",
    "# Input: diamond, usearch results, ARO Mapping file\n",
    "# Output: Processed integrated search results\n",
    "\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os, pathlib, subprocess\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "github = base / \".github\"\n",
    "log_dir = base / \"logs\"\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "silva_dir = github / \"data/silva_db\"\n",
    "card_dir = github / \"data/card_db\"\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "silva_res = result_dir / \"SILVA_results.txt\"\n",
    "card_res = result_dir / \"card_results.txt\"\n",
    "aro_file = \"aro_index.tsv\"\n",
    "aro_path = card_dir / \"aro_index.tsv\"\n",
    "aro_tar = card_dir / \"card_seq.tar.bz2\"\n",
    "card_interm = result_dir / \"card_intermed.csv\"\n",
    "silva_interm = result_dir / \"silva_intermed.csv\"\n",
    "result = result_dir / \"integrated_result.csv\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd):\n",
    "    result = subprocess.run(cmd, shell=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "\n",
    "# === Extract Aro ===\n",
    "run(f\"tar -xvjf {aro_tar} ./{aro_file}; mv {aro_file} {card_dir}\")\n",
    "\n",
    "def classify_db(subj):\n",
    "    # CARD entries have 4 parts when split by \"|\" and contain \"ARO:<number>\"\n",
    "    if isinstance(subj, str) and len(subj.split(\"|\")) == 4 and re.search(r\"ARO:\\d+\", subj):\n",
    "        return \"card\"\n",
    "    # UniRef entries contain \"UniRef\" and an underscore followed by a number\n",
    "    elif isinstance(subj, str) and re.search(r\"UniRef\\d+_\", subj):\n",
    "        return \"uniref\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# === Integrate Script ===\n",
    "def process_card_results(card_path, aro_path, blast_columns, output_path):\n",
    "    \"\"\"Process CARD results and save them to an intermediate output file\"\"\"\n",
    "    aro_df = pd.read_csv(aro_path, sep=\"\\t\")\n",
    "\n",
    "    with gzip.open(card_path, \"rt\") as f_in, open(output_path, \"w\") as f_out:\n",
    "        card_df = pd.read_csv(f_in,compression=\"gzip\", sep=\"\\t\", names=blast_columns)\n",
    "        card_df[\"part\"] = \"ABR\"\n",
    "        card_df[\"db\"] = card_df[\"subject_id\"].apply(classify_db)\n",
    "        # Extract ARO accession (formatted like: ARO|...|ACCESSION|...)\n",
    "        card_df[\"ARO Accession\"] = card_df[\"subject_id\"].str.split(\n",
    "            \"|\", expand=True\n",
    "        )[2]\n",
    "        merged_df = card_df.merge(aro_df, on=\"ARO Accession\", how=\"left\")\n",
    "        merged_df.to_csv(f_out, index=False)\n",
    "\n",
    "\n",
    "def process_silva_results(silva_path, blast_columns, output_path):\n",
    "    \"\"\"Process SILVA results and save them to an intermediate output file.\"\"\"\n",
    "\n",
    "    with gzip.open(silva_path, \"rt\") as f_in, open(output_path, \"w\") as f_out:\n",
    "        silva_df = pd.read_csv(f_in,compression=\"gzip\", sep=\"\\t\", names=blast_columns)\n",
    "        silva_df[\"part\"] = \"16S\"\n",
    "        # Extract the primary accession (before '.') from SILVA subject_id\n",
    "        silva_df[\"primaryAccession\"] = silva_df[\"subject_id\"].str.split(\n",
    "            \".\", expand=True\n",
    "        )[0]\n",
    "        silva_df[\"genus\"] = silva_df[\"subject_id\"].str.split(\";\").str[-2]\n",
    "        silva_df.to_csv(f_out, index=False)\n",
    "\n",
    "\n",
    "def merge_results(card_output, silva_output, final_output):\n",
    "    \"\"\"Merge processed CARD and SILVA results into one final output file and update overview\"\"\"\n",
    "    card_df = pd.read_csv(card_output)\n",
    "    silva_df = pd.read_csv(silva_output)\n",
    "\n",
    "    combined_df = pd.concat([silva_df, card_df])\n",
    "    combined_df.to_csv(final_output, index=False)\n",
    "\n",
    "    # Count number of rows in the combined DataFrame\n",
    "    count = len(combined_df)\n",
    "\n",
    "    print(f\"Merged similarity hits,{count}\\n\")\n",
    "\n",
    "blast_columns = [\n",
    "    \"query_id\",\n",
    "    \"subject_id\",\n",
    "    \"perc_identity\",\n",
    "    \"align_length\",\n",
    "    \"mismatches\",\n",
    "    \"gap_opens\",\n",
    "    \"q_start\",\n",
    "    \"q_end\",\n",
    "    \"s_start\",\n",
    "    \"s_end\",\n",
    "    \"evalue\",\n",
    "    \"bit_score\",\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_card = executor.submit(\n",
    "        process_card_results, card_res, aro_path, blast_columns, card_interm\n",
    "    )\n",
    "    future_silva = executor.submit(\n",
    "        process_silva_results, silva_res, blast_columns, silva_interm\n",
    "    )\n",
    "\n",
    "    future_card.result()\n",
    "    future_silva.result()\n",
    "\n",
    "merge_results(card_interm, silva_interm, result)\n",
    "\n",
    "# === Cleanup ===\n",
    "#run(f\"rm {result_dir}/*intermed*; rm {card_dir}/aro*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filter Blast results\n",
    "# Selfwritten python script \"filter_blast_results.py\"\n",
    "# Input: integrated_filtered_results.csv\n",
    "# Output: filtered_results.csv\n",
    "\n",
    "import pandas as pd\n",
    "import os, pathlib, subprocess\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "merge_result = result_dir / \"integrated_result.csv\"\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "\n",
    "# === Utils ===\n",
    "def run(cmd):\n",
    "    result = subprocess.run(cmd, shell=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "    \n",
    "# === Filter Script ===\n",
    "dtype_dict = {\n",
    "    \"query_id\": \"string\",\n",
    "    \"subject_id\": \"string\",\n",
    "    \"perc_identity\": \"float\",\n",
    "    \"align_length\": \"int\",\n",
    "    \"evalue\": \"float\",\n",
    "    \"part\": \"string\",\n",
    "    \"genus\": \"string\",\n",
    "    \"AMR Gene Family\": \"string\",\n",
    "    \"Drug Class\": \"string\",\n",
    "    \"ARO Name\": \"string\",\n",
    "    \"db\": \"string\",\n",
    "}\n",
    "\n",
    "\n",
    "def read_input_data(input_file):\n",
    "    \"\"\"Load relevant columns from input file with proper dtypes\"\"\"\n",
    "    return pd.read_csv(input_file, sep=\",\", dtype=dtype_dict, usecols=dtype_dict.keys())\n",
    "\n",
    "\n",
    "def filter_by_identity(df, part, min_similarity):\n",
    "    \"\"\"Filter BLAST result for either ABR and 16S part based on percent identity\"\"\"\n",
    "    data_pre = df[df[\"part\"] == part]\n",
    "    filtered = data_pre[data_pre[\"perc_identity\"] > min_similarity * 100]\n",
    "    filtered_count = len(data_pre) - len(filtered)\n",
    "    return filtered, filtered_count\n",
    "\n",
    "\n",
    "def keep_max_identity_per_query(df):\n",
    "    \"\"\"For each query_id, keep only rows with the highest percent identity\"\"\"\n",
    "    max_identities = df.groupby(\"query_id\")[\"perc_identity\"].max().reset_index()\n",
    "    merged = df.merge(max_identities, on=[\"query_id\", \"perc_identity\"])\n",
    "    return merged\n",
    "\n",
    "def keep_best_per_query(df):\n",
    "    \"\"\"For each query_id, keep the row with the highest perc_identity and lowest evalue\"\"\"\n",
    "    return (\n",
    "        df.sort_values(\n",
    "            by=[\"query_id\"] + [\"perc_identity\", \"evalue\"], \n",
    "            ascending=[True,False, True]\n",
    "            ).drop_duplicates(subset=\"query_id\", keep=\"first\")\n",
    "    )\n",
    "\n",
    "def clean_16s_query_ids(df):\n",
    "    \"\"\"Remove anything after the first whitespace in 16S query IDs\"\"\"\n",
    "    df[\"query_id\"] = df[\"query_id\"].str.split().str[0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_parts_on_query_id(abr_data, s16_data):\n",
    "    \"\"\"Return only rows with query_ids present in both ABR and 16S data\"\"\"\n",
    "    common_ids = pd.Index(abr_data[\"query_id\"]).intersection(s16_data[\"query_id\"])\n",
    "    return (\n",
    "        abr_data[abr_data[\"query_id\"].isin(common_ids)],\n",
    "        s16_data[s16_data[\"query_id\"].isin(common_ids)],\n",
    "    )\n",
    "\n",
    "def write_summary(sample, stats):\n",
    "    \"\"\"Write all filtering summary statistics to the overview file\"\"\"\n",
    "    if overview_table.is_file():\n",
    "        for stat_name, value in stats.items():\n",
    "            print(f\"{sample},{stat_name},{value}\")\n",
    "    else:\n",
    "        with open(overview_table, \"a\") as file:\n",
    "            for stat_name, value in stats.items():\n",
    "                file.write(f\"{sample},{stat_name},{value}\\n\")\n",
    "                print(f\"{sample},{stat_name},{value}\")        \n",
    "\n",
    "def rename_for_merge(df,part):\n",
    "    df_renamed = df.rename(columns={\n",
    "        \"perc_identity\": \"perc_identity_\"+part,\n",
    "        \"subject_id\": \"subject_id_\"+part,\n",
    "        \"align_length\": \"align_length_\"+part,\n",
    "        \"evalue\": \"evalue_\"+part,\n",
    "    })\n",
    "    return df_renamed\n",
    "\n",
    "def filter_blast_results(input_file, output_file, min_similarity):\n",
    "    \"\"\"Main filtering logic for BLAST results across ABR and 16S data parts\"\"\"\n",
    "    df = read_input_data(input_file)\n",
    "\n",
    "    # ABR filtering\n",
    "    abr_threshold_filtered, abr_removed_identity = filter_by_identity(df, \"ABR\", min_similarity)\n",
    "    abr_best_identity = keep_max_identity_per_query(abr_threshold_filtered)\n",
    "    abr_best_query = keep_best_per_query(abr_best_identity)\n",
    "    abr_final = rename_for_merge(abr_best_query ,\"ABR\")\n",
    "    abr_removed_max = len(abr_threshold_filtered) - len(abr_final)\n",
    "\n",
    "    # 16S filtering\n",
    "    s16_threshold_filtered, s16_removed_identity = filter_by_identity(df, \"16S\", min_similarity)\n",
    "    s16_cleaned = clean_16s_query_ids(s16_threshold_filtered)\n",
    "    s16_best_identity = keep_max_identity_per_query(s16_cleaned)\n",
    "    s16_best_query = keep_best_per_query(s16_best_identity)\n",
    "    s16_final = rename_for_merge(s16_best_query,\"16S\")\n",
    "    s16_removed_max = len(s16_threshold_filtered) - len(s16_final)\n",
    "\n",
    "    # Match ABR and 16S by query_id\n",
    "    abr_common, s16_common = merge_parts_on_query_id(abr_final, s16_final)\n",
    "    removed_query_id_mismatch = (len(abr_final) + len(s16_final)) - (\n",
    "        len(abr_common)\n",
    "    )\n",
    "    uni_abr_ids = abr_final.loc[\n",
    "        abr_final[\"subject_id_ABR\"].str.contains(\"Uni\"), \"query_id\"\n",
    "    ]\n",
    "\n",
    "    matches_in_16s = s16_final[\"query_id\"].isin(uni_abr_ids)\n",
    "    print(\"Matches in 16S:\", matches_in_16s.sum(), \"/\", len(uni_abr_ids))\n",
    "\n",
    "    # Merge side-by-side on query_id\n",
    "    merged = pd.merge(\n",
    "        abr_common[[\"query_id\", \"AMR Gene Family\", \"perc_identity_ABR\", \"align_length_ABR\", \"evalue_ABR\", \"Drug Class\", \"ARO Name\", \"subject_id_ABR\",\"db\"]],\n",
    "        s16_common[[\"query_id\", \"subject_id_16S\",\"genus\", \"perc_identity_16S\", \"align_length_16S\", \"evalue_16S\"]],\n",
    "        on=\"query_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    merged.to_csv(output_file, index=False)\n",
    "\n",
    "    # Extract sample and part from file path\n",
    "    sample = \"test_epic_data\"\n",
    "\n",
    "    # Write summary\n",
    "    stats = {\n",
    "        \"Diamond hits < similarity threshold\": \"-\" + str(abr_removed_identity),\n",
    "        \"Diamond hits NOT highest percentage identity per query\": \"-\" + str(abr_removed_max),\n",
    "        \"Usearch hits < similarity threshold\": \"-\" + str(s16_removed_identity),\n",
    "        \"Usearch hits NOT highest percentage identity per query\": \"-\" + str(s16_removed_max),\n",
    "        \"Query hit in only one of two databases\": \"-\" + str(removed_query_id_mismatch),\n",
    "        \"Filtered fusion reads\": len(merged),\n",
    "    }\n",
    "    write_summary(sample, stats)\n",
    "\n",
    "filter_blast_results(merge_result, filter_result, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create abundance table\n",
    "# Selfwritten python script \"generate_genus_distribution_table.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: abundance plot over all ABRs\n",
    "\n",
    "import pandas as pd\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "\n",
    "# === Abundance Table Script ===\n",
    "\n",
    "def process_combined_data(combined_data, sample_name):\n",
    "    combined_data[\"sample\"] = sample_name\n",
    "\n",
    "    # Count genus occurrences per AMR Gene Family\n",
    "    genus_counts = (\n",
    "        combined_data.groupby(\n",
    "            [\n",
    "                \"sample\",\n",
    "                \"AMR Gene Family\",\n",
    "                \"genus\",\n",
    "                \"Drug Class\",\n",
    "                \"ARO Name\",\n",
    "                \"db\"\n",
    "            ]\n",
    "        )\n",
    "        .size()\n",
    "        .reset_index(name=\"genus_count\")\n",
    "    )\n",
    "\n",
    "    # Calculate total genus count per AMR Gene Family within each sample\n",
    "    total_counts = (\n",
    "        genus_counts.groupby([\"sample\", \"AMR Gene Family\"])[\"genus_count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_count\")\n",
    "    )\n",
    "\n",
    "    # Join and calculate relative abundance\n",
    "    result = pd.merge(genus_counts, total_counts, on=[\"sample\", \"AMR Gene Family\"])\n",
    "    result[\"relative_genus_count\"] = round(\n",
    "        result[\"genus_count\"] / result[\"total_count\"], 4\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def load_and_merge_parts(file_list):\n",
    "    \"\"\"Load and merges dataframes from compressed CSV files\"\"\"\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file due to read error [{file}]: {repr(e)}\")\n",
    "    if data_frames:\n",
    "        merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        merged_df = pd.DataFrame()\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def export_genera_abundance(input_files, output_path):\n",
    "    \"\"\"Group input files by sample\"\"\"\n",
    "    sample_to_files = {}\n",
    "    for file in [str(input_files)]:\n",
    "        # Extract sample name from the file path, assuming 3rd-to-last split is the sample name\n",
    "        sample = \"test_epic_data\"\n",
    "        sample_to_files.setdefault(sample, []).append(file)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for sample_name, files in sample_to_files.items():\n",
    "        merged_data = load_and_merge_parts(files)\n",
    "        sample_data = process_combined_data(merged_data, sample_name)\n",
    "        all_data.append(sample_data)\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=[\"sample\",\"AMR Gene Family\",\"genus_count\"], ascending=False)\n",
    "\n",
    "    # Export the final aggregated data to a CSV file\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    display(final_df)\n",
    "\n",
    "export_genera_abundance(filter_result, abundance_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create stacked bar abundance plot\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: abundance file\n",
    "# Output: bubble plot per sample\n",
    "\n",
    "import os, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "abundance_bar_plot = result_dir / \"combined_genus_abundance_barplot.html\"\n",
    "\n",
    "# ─── Constants ─────────────────────────────────────────────────────────\n",
    "RESERVED_COLOR = 'rgb(217,217,217)'\n",
    "AMR_MIN_FRACTION = 0.01\n",
    "\n",
    "def get_genus_colors(all_genera):\n",
    "    \"\"\"Assign consistent, distinguishable colors to each genus.\"\"\"\n",
    "    top_colors = [\n",
    "        '#D62728',  # dark red\n",
    "        '#FF7F0E',  # orange\n",
    "        '#8B4513',  # brown\n",
    "        '#1F77B4',  # dark blue\n",
    "        '#800080',  # purple\n",
    "        '#7F7F7F',  # gray\n",
    "        '#2CA02C',  # dark green\n",
    "        '#1E90FF',  # blue\n",
    "        '#BA55D3',  # medium orchid\n",
    "        '#BCBD22',  # yellow-green\n",
    "    ]\n",
    "\n",
    "    fallback_palette = (\n",
    "        px.colors.qualitative.Pastel +\n",
    "        px.colors.qualitative.Set3 +\n",
    "        px.colors.qualitative.Alphabet +\n",
    "        px.colors.qualitative.Light24 +\n",
    "        px.colors.qualitative.Bold\n",
    "    )\n",
    "\n",
    "    # Remove duplicates and reserved color from palette\n",
    "    color_pool = list(dict.fromkeys(top_colors + fallback_palette))\n",
    "    if RESERVED_COLOR in color_pool:\n",
    "        color_pool.remove(RESERVED_COLOR)\n",
    "\n",
    "    # Assign genera with a unique color each\n",
    "    genus_list = [g for g in all_genera if g != \"Others\"]\n",
    "    if len(genus_list) > len(color_pool):\n",
    "        raise ValueError(f\"Too many genera ({len(genus_list)}) for available color pool.\")\n",
    "    genus_colors = {g: color_pool[i] for i, g in enumerate(genus_list)}\n",
    "    genus_colors[\"Others\"] = RESERVED_COLOR\n",
    "    return genus_colors\n",
    "\n",
    "def preprocess_abundance(df, amr, min_genus_abundance, force_include, force_exclude):\n",
    "    \"\"\"Filter and aggregate genus abundance data for a given AMR family.\"\"\"\n",
    "    df_amr = df[df[\"AMR Gene Family\"] == amr].copy()\n",
    "\n",
    "    # Determine low-abundance or excluded genera\n",
    "    low_abundance = df_amr[\n",
    "        ((df_amr[\"relative_genus_count\"] <= min_genus_abundance) & (~df_amr[\"genus\"].isin(force_include))) |\n",
    "        (df_amr[\"genus\"].isin(force_exclude))\n",
    "    ]\n",
    "    others = (\n",
    "        low_abundance.groupby(['sample', 'total_count'], as_index=False)\n",
    "        .agg({\"relative_genus_count\": \"sum\"})\n",
    "        .assign(genus=\"Others\")\n",
    "    )\n",
    "    others[\"sample_label\"] = others[\"sample\"] + \" (\" + others[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    # Remove excluded genera\n",
    "    df_amr = df_amr[~df_amr[\"genus\"].isin(force_exclude)]\n",
    "    df_amr = df_amr.sort_values(by=['sample','AMR Gene Family','genus_count'],ascending=[True,False,False])\n",
    "    # plot high abundance or forced-includes\n",
    "    df_amr_filtered = df_amr[\n",
    "        (df_amr[\"relative_genus_count\"] > min_genus_abundance) | (df_amr[\"genus\"].isin(force_include))\n",
    "    ]\n",
    "\n",
    "    # Add \"Others\"\n",
    "    df_final = pd.concat([df_amr_filtered, others], ignore_index=True)\n",
    "    df_final[\"sample_label\"] = df_final[\"sample\"] + \" (\" + df_final[\"total_count\"].astype(str) + \")\"\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def plot_stacked_abundance(\n",
    "    observed_csv,\n",
    "    output_html,\n",
    "    min_genus_abundance,\n",
    "    force_include=None,\n",
    "    force_exclude=None,\n",
    "    min_reads=20\n",
    "):\n",
    "\n",
    "    force_include = force_include or []\n",
    "    force_exclude = force_exclude or []\n",
    "\n",
    "    df = pd.read_csv(observed_csv)\n",
    "    df = df.sort_values([\"sample\", \"genus_count\"], ascending=[True, False])\n",
    "    df = df[df[\"total_count\"] > min_reads]\n",
    "    amr_totals = df.groupby(\"AMR Gene Family\")[\"total_count\"].sum()\n",
    "    total_all = amr_totals.sum()\n",
    "    amrs_to_plot = amr_totals[amr_totals >= total_all * AMR_MIN_FRACTION].index.tolist()\n",
    "\n",
    "    if not amrs_to_plot:\n",
    "        print(\"No AMR Gene Families meet the abundance threshold.\")\n",
    "        return\n",
    "\n",
    "    df = df[df[\"AMR Gene Family\"].isin(amrs_to_plot)]\n",
    "    amrs = sorted(df[\"AMR Gene Family\"].unique())\n",
    "    samples = df[\"sample\"].nunique()\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(amrs),\n",
    "        cols=1,\n",
    "        subplot_titles=amrs,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    for i, amr in enumerate(amrs, start=1):\n",
    "        df_amr = preprocess_abundance(\n",
    "            df, amr, min_genus_abundance, force_include, force_exclude\n",
    "        )\n",
    "        genus_colors = get_genus_colors(df_amr[\"genus\"].unique())\n",
    "\n",
    "        legendgroup = f\"group{i}\"  # unique group per subplot\n",
    "        for genus in df_amr[\"genus\"].unique():\n",
    "            genus_data = df_amr[df_amr[\"genus\"] == genus]\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=genus_data[\"sample_label\"],\n",
    "                    y=genus_data[\"relative_genus_count\"],\n",
    "                    name=genus,\n",
    "                    marker_color=genus_colors[genus],\n",
    "                    legendgroup=legendgroup,\n",
    "                    legendgrouptitle=dict(text=amr) if genus == df_amr[\"genus\"].unique()[0] else None,\n",
    "                    showlegend=True,\n",
    "                ),\n",
    "                row=i,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        # Custom legend positioning for each subplot (optional, only needed if separating legends visually)\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                y=1,\n",
    "                yanchor=\"top\",\n",
    "                x=2.5-np.log10(samples),\n",
    "                xanchor=\"left\",\n",
    "                tracegroupgap=500  # adds spacing between legend groups\n",
    "            ),\n",
    "            margin=dict(r=300)  # enough space for long legends\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\",\n",
    "        title=\"Relative Genus Abundance per AMR Gene Family\",\n",
    "        height=800 * len(amrs),\n",
    "        width=1000 * np.log10(samples) if samples > 2 else 500,\n",
    "        plot_bgcolor=\"white\",\n",
    "        yaxis=dict(tickformat=\".0%\"),\n",
    "        showlegend=True,\n",
    "        margin=dict(r=300),\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Relative Abundance\",categoryorder=\"array\",categoryarray=sorted(df_amr[\"sample_label\"].unique()))\n",
    "\n",
    "    fig.show()\n",
    "    # fig.write_html(output_html)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"\"\n",
    "    min_abundance = 0.01\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    plot_stacked_abundance(input_csv, output_html, float(min_abundance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "RESERVED_COLOR = \"rgb(217,217,217)\"\n",
    "\n",
    "def get_genus_colors(all_genera):\n",
    "    \"\"\"Assign consistent, distinguishable colors to each genus.\"\"\"\n",
    "    top_colors = [\n",
    "        \"#D62728\", \"#FF7F0E\", \"#8B4513\", \"#1F77B4\", \"#800080\",\n",
    "        \"#7F7F7F\", \"#2CA02C\", \"#1E90FF\", \"#BA55D3\", \"#BCBD22\"\n",
    "    ]\n",
    "\n",
    "    fallback_palette = (\n",
    "        px.colors.qualitative.Pastel\n",
    "        + px.colors.qualitative.Set3\n",
    "        + px.colors.qualitative.Alphabet\n",
    "        + px.colors.qualitative.Light24\n",
    "        + px.colors.qualitative.Bold\n",
    "    )\n",
    "\n",
    "    color_pool = list(dict.fromkeys(top_colors + fallback_palette))\n",
    "    if RESERVED_COLOR in color_pool:\n",
    "        color_pool.remove(RESERVED_COLOR)\n",
    "\n",
    "    genus_list = [g for g in all_genera if g != \"Others\"]\n",
    "    if len(genus_list) > len(color_pool):\n",
    "        raise ValueError(\n",
    "            f\"Too many genera ({len(genus_list)}) for available color pool.\"\n",
    "        )\n",
    "\n",
    "    genus_colors = {g: color_pool[i] for i, g in enumerate(genus_list)}\n",
    "    genus_colors[\"Others\"] = RESERVED_COLOR\n",
    "    return genus_colors\n",
    "\n",
    "\n",
    "def preprocess_abundance(df, min_genus_abundance, force_include, force_exclude):\n",
    "    \"\"\"Filter and aggregate genus abundance data.\"\"\"\n",
    "    df_amr = df.copy()\n",
    "\n",
    "    # Group low-abundance or excluded genera into 'Others'\n",
    "    low_abundance = df_amr[\n",
    "        ((df_amr[\"relative_genus_count\"] <= min_genus_abundance) & ~df_amr[\"genus\"].isin(force_include))\n",
    "        | df_amr[\"genus\"].isin(force_exclude)\n",
    "    ]\n",
    "    others = (\n",
    "        low_abundance.groupby([\"sample\", \"total_count\"], as_index=False)\n",
    "        .agg({\"relative_genus_count\": \"sum\"})\n",
    "        .assign(genus=\"Others\")\n",
    "    )\n",
    "    others[\"sample_label\"] = others[\"sample\"] + \" (\" + others[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    # Keep only included & high-abundance genera\n",
    "    df_amr = df_amr[~df_amr[\"genus\"].isin(force_exclude)]\n",
    "    df_amr_filtered = df_amr[\n",
    "        (df_amr[\"relative_genus_count\"] > min_genus_abundance)\n",
    "        | df_amr[\"genus\"].isin(force_include)\n",
    "    ]\n",
    "    df_amr_filtered = df_amr_filtered.sort_values([\"sample\", \"AMR Gene Family\", \"genus_count\"], ascending=[True, False, False])\n",
    "    df_amr_filtered[\"sample_label\"] = df_amr_filtered[\"sample\"] + \" (\" + df_amr_filtered[\"total_count\"].astype(str) + \")\"\n",
    "\n",
    "    return pd.concat([df_amr_filtered, others], ignore_index=True)\n",
    "\n",
    "\n",
    "def plot_stacked_abundance(\n",
    "    observed_csv,\n",
    "    output_html=None,\n",
    "    min_genus_abundance=0.01,\n",
    "    force_include=None,\n",
    "    force_exclude=None,\n",
    "):\n",
    "    \"\"\"Plot a single stacked bar chart of genus abundance for most prevalent AMR per sample.\"\"\"\n",
    "    force_include = force_include or []\n",
    "    force_exclude = force_exclude or []\n",
    "\n",
    "    df = pd.read_csv(observed_csv)\n",
    "    amr = df.groupby(\"AMR Gene Family\")[\"total_count\"].sum().idxmax()\n",
    "    print(amr)\n",
    "    # Keep only AMR with highest total_count per sample\n",
    "    df_max = df[df[\"AMR Gene Family\"] == amr]\n",
    "    display(df_max)\n",
    "    amrs = df_max[\"AMR Gene Family\"].unique()\n",
    "    samples = df_max[\"sample\"].nunique()\n",
    "\n",
    "    df_amr = preprocess_abundance(df_max, min_genus_abundance, force_include, force_exclude)\n",
    "    genus_colors = get_genus_colors(df_amr[\"genus\"].unique())\n",
    "\n",
    "    fig = go.Figure()\n",
    "    legend_added = set()\n",
    "\n",
    "    for genus in df_amr[\"genus\"].unique():\n",
    "        genus_data = df_amr[df_amr[\"genus\"] == genus]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=genus_data[\"sample_label\"],\n",
    "                y=genus_data[\"relative_genus_count\"],\n",
    "                name=genus,\n",
    "                marker_color=genus_colors[genus],\n",
    "                showlegend=genus not in legend_added,\n",
    "            )\n",
    "        )\n",
    "        legend_added.add(genus)\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\",\n",
    "        title=f\"Relative Genus Abundance (Most Abundant AMR Gene Family: {', '.join(amrs)})\",\n",
    "        height=800,\n",
    "        width=1000 * np.log10(samples) if samples > 2 else 500,\n",
    "        plot_bgcolor=\"white\",\n",
    "        legend_title=\"Genus\",\n",
    "        xaxis=dict(tickangle=45),\n",
    "        yaxis=dict(title=\"Relative Abundance\", tickformat=\".0%\"),\n",
    "    )\n",
    "\n",
    "    if output_html:\n",
    "        fig.write_html(output_html)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"\"\n",
    "    min_abundance = 0.01\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    plot_stacked_abundance(input_csv, output_html, float(min_abundance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create bubble plot\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: abundance file\n",
    "# Output: bubble plot per sample\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "abundance_result = result_dir / \"genera_abundance.csv\"\n",
    "bubble_plot = result_dir / \"combined_genus_abundance_bubbleplot.html\"\n",
    "\n",
    "# === Bubble Plot Script ===\n",
    "def load_filtered_data(input_csv, min_total_count=100):\n",
    "    \"\"\"Load CSV and filter AMR Gene Families by minimum total genus count\"\"\"\n",
    "    df = pd.read_csv(input_csv, sep=\",\")\n",
    "    return df[df[\"total_count\"] > min_total_count]\n",
    "\n",
    "\n",
    "def get_top_genera_per_sample(df, top_n):\n",
    "    \"\"\"Return dicts of top genera per sample (set and list forms)\"\"\"\n",
    "    top_sets = {}\n",
    "    top_lists = {}\n",
    "    for sample in df[\"sample\"].unique():\n",
    "        sample_df = df[df[\"sample\"] == sample]\n",
    "        top = (\n",
    "            sample_df.sort_values(by=\"relative_genus_count\", ascending=False)\n",
    "            .head(top_n)[\"genus\"]\n",
    "            .tolist()\n",
    "        )\n",
    "        top_sets[sample] = set(top)\n",
    "        top_lists[sample] = top\n",
    "    return top_sets, top_lists\n",
    "\n",
    "\n",
    "def select_genera(top_sets, top_lists, max_genera, min_overlap):\n",
    "    \"\"\"Select a list of genera to display using overlap or merged ranking\"\"\"\n",
    "    if not top_sets:\n",
    "        return []\n",
    "\n",
    "    overlap = set.intersection(*top_sets.values())\n",
    "    total_genus = sum(len(lst) for lst in top_lists.values())\n",
    "\n",
    "    if len(overlap) >= min_overlap:\n",
    "        return list(overlap)[:max_genera]\n",
    "    elif total_genus > max_genera:\n",
    "        combined = set(overlap)\n",
    "        sample_iters = {s: iter(l) for s, l in top_lists.items()}\n",
    "\n",
    "        while len(combined) < max_genera:\n",
    "            for gen_iter in sample_iters.values():\n",
    "                try:\n",
    "                    while True:\n",
    "                        genus = next(gen_iter)\n",
    "                        if genus not in combined:\n",
    "                            combined.add(genus)\n",
    "                            break\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                if len(combined) >= max_genera:\n",
    "                    break\n",
    "        return list(combined)\n",
    "    else:\n",
    "        return list({genus for sublist in top_lists.values() for genus in sublist})\n",
    "\n",
    "\n",
    "def add_amr_family_subplot(\n",
    "    fig, df, amr_family, col_idx, max_genera, min_overlap, top_per_sample\n",
    "):\n",
    "    \"\"\"Filter and add a subplot for one AMR Gene Family to the main figure\"\"\"\n",
    "    df_amr = df[df[\"AMR Gene Family\"] == amr_family]\n",
    "    if df_amr.empty:\n",
    "        return\n",
    "\n",
    "    top_sets, top_lists = get_top_genera_per_sample(df_amr, top_per_sample)\n",
    "    selected = select_genera(top_sets, top_lists, max_genera, min_overlap)\n",
    "    df_plot = df_amr[df_amr[\"genus\"].isin(selected)]\n",
    "\n",
    "    scatter = px.scatter(\n",
    "        df_plot,\n",
    "        x=\"sample\",\n",
    "        y=\"genus\",\n",
    "        size=\"relative_genus_count\",\n",
    "        color=\"total_count\",\n",
    "        hover_name=\"genus\",\n",
    "        hover_data={\n",
    "            \"genus_count\": True,\n",
    "            \"relative_genus_count\": True,\n",
    "            \"total_count\": True,\n",
    "            \"sample\": False,\n",
    "        },\n",
    "        size_max=20,\n",
    "        color_continuous_scale=\"Greens\",\n",
    "    )\n",
    "\n",
    "    for trace in scatter.data:\n",
    "        fig.add_trace(trace, row=1, col=col_idx)\n",
    "\n",
    "\n",
    "def create_bubble_plot_grid(df, max_genera, min_overlap, top_per_sample):\n",
    "    \"\"\"Create the full multi-subplot bubble chart\"\"\"\n",
    "    families = df[\"AMR Gene Family\"].unique()\n",
    "    num_cols = len(families) if len(df) > 1 else 1\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=num_cols,\n",
    "        subplot_titles=list(families),\n",
    "        horizontal_spacing=0.2,\n",
    "    )\n",
    "\n",
    "    for idx, family in enumerate(families, start=1):\n",
    "        add_amr_family_subplot(\n",
    "            fig, df, family, idx, max_genera, min_overlap, top_per_sample\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Bubble Plots of Top Genera for Each AMR Gene Family\",\n",
    "        plot_bgcolor=\"lightgrey\",\n",
    "        height=900,\n",
    "        width=500 * num_cols,\n",
    "        coloraxis_colorbar=dict(title=\"Fusion Read Count\"),\n",
    "    )\n",
    "    fig.update_yaxes(categoryorder=\"category descending\")\n",
    "    fig.update_xaxes(categoryorder=\"category ascending\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_bubble_plots_combined(\n",
    "    input_csv, output_html, max_genera=20, min_overlap=10, top_per_sample=20\n",
    "):\n",
    "    \"\"\"Load input, pass to processing function and save plot\"\"\"\n",
    "    df = load_filtered_data(input_csv)\n",
    "    fig = create_bubble_plot_grid(df, max_genera, min_overlap, top_per_sample)\n",
    "    pio.write_html(fig, file=output_html)\n",
    "    pio.show(fig)\n",
    "\n",
    "create_bubble_plots_combined(abundance_result, bubble_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create boxplots\n",
    "# Selfwritten python script \"percidt_per_genus.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: boxplot over all samples per percentage identity, number of unique hits and genera\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "result_dir = base / \".test_steps/results\"\n",
    "\n",
    "filter_result = result_dir / \"filtered_result.csv\"\n",
    "boxplot = result_dir / \"genus_idt_per_genus_plot.png\"\n",
    "\n",
    "\n",
    "def generate_percentage_idt_per_genus(input_files, output_file):\n",
    "    all_data = []  # List to hold DataFrames from all input files\n",
    "\n",
    "    for input_file in [str(input_files)]:\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            sep=\",\",\n",
    "            header=0,\n",
    "        )\n",
    "        all_data.append(df)\n",
    "\n",
    "    # Combine all partitions into a single DataFrame\n",
    "    combined_data = pd.concat(all_data)\n",
    "\n",
    "    # Calculate genus query counts\n",
    "    genus_query_counts = (\n",
    "        combined_data.groupby(\"genus\")[\"query_id\"].nunique().reset_index()\n",
    "    )\n",
    "    genus_query_counts.columns = [\"genus\", \"unique_query_count\"]\n",
    "\n",
    "    # Keep only the top 20 genera\n",
    "    top20_species = genus_query_counts.nlargest(20, \"unique_query_count\")\n",
    "\n",
    "    # Filter combined_data to retain only the top 20 genera\n",
    "    combined_data = combined_data[combined_data[\"genus\"].isin(top20_species[\"genus\"])]\n",
    "\n",
    "    # Now filter genus_query_counts as well\n",
    "    genus_query_counts = genus_query_counts[\n",
    "        genus_query_counts[\"genus\"].isin(top20_species[\"genus\"])\n",
    "    ]\n",
    "\n",
    "    # Define order for the x-axis\n",
    "    genus_order = top20_species.sort_values(by=\"unique_query_count\", ascending=False)[\n",
    "        \"genus\"\n",
    "    ]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    sns.boxplot(\n",
    "        x=\"genus\",\n",
    "        y=\"perc_identity_16S\",\n",
    "        data=combined_data,\n",
    "        ax=ax1,\n",
    "        order=genus_order,\n",
    "        fliersize=0.0,\n",
    "        color=\"dodgerblue\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Bacterial Genus\")\n",
    "    ax1.set_ylabel(\"Percentage Identity (boxplot)\", color=\"royalblue\")\n",
    "    ax1.set_title(\n",
    "        \"Boxplot of Percentage Identity and Read Counts for Each Bacterial Genus\"\n",
    "    )\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Add a second y-axis for unique query counts\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.barplot(\n",
    "        x=\"genus\",\n",
    "        y=\"unique_query_count\",\n",
    "        data=genus_query_counts,\n",
    "        ax=ax2,\n",
    "        alpha=0.2,\n",
    "        color=\"purple\",\n",
    "        order=genus_order,\n",
    "    )\n",
    "    ax2.set_ylabel(\"Number of hits (bar)\", color=\"violet\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "generate_percentage_idt_per_genus(filter_result, boxplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create boxplots\n",
    "# Selfwritten python scripts \"boxplot_[align_lengths,evalue,percidt].py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: boxplot over all samples per parameter alignment lengths, E-value or percentage identity\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "This script takes a list of all filtered fasta files, combines e-value information \n",
    "across samples, and visualizes the distribution of e-values using boxplots split \n",
    "by part (ABR/16S) and sample.\n",
    "\"\"\"\n",
    "\n",
    "PRETTY_LABELS = {\n",
    "    \"align_length\": \"Alignment length\",\n",
    "    \"perc_identity\": \"Percentage identity\",\n",
    "    \"evalue\": \"E-value\"\n",
    "}\n",
    "\n",
    "def read_and_process_partitioned_data(partition_files, sample, param):\n",
    "    \"\"\"Read and process partitioned files for a single sample.\"\"\"\n",
    "    data_frames = []\n",
    "    sample_name = sample\n",
    "    param = param\n",
    "    for part_file in partition_files:\n",
    "        if os.path.exists(part_file):\n",
    "            df = pd.read_csv(\n",
    "                part_file, header=0, sep=\",\"\n",
    "            )\n",
    "            #df[f\"{param}_ABR\"] = df[f\"{param}_ABR\"] * 3\n",
    "            long_df = pd.melt(\n",
    "                df,\n",
    "                id_vars=[\"query_id\"],\n",
    "                value_vars=[param + \"_ABR\", param + \"_16S\"],\n",
    "                var_name=\"part\",\n",
    "                value_name=param\n",
    "            )\n",
    "\n",
    "            # Normalize part labels\n",
    "            long_df[\"part\"] = long_df[\"part\"].str.replace(param + \"_\", \"\")\n",
    "            long_df[\"sample\"] = sample_name\n",
    "            data_frames.append(long_df)\n",
    "        \n",
    "    if data_frames:\n",
    "        return pd.concat(data_frames)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def plot_boxplots(data, output_file):\n",
    "    \"\"\"\n",
    "    Generate and save boxplots of e-values across samples and parts (ABR vs. 16S).\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Combined dataframe containing 'sample', 'evalue', and 'part'.\n",
    "        output_file (str): Path to save the resulting plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    flierprops = dict(markerfacecolor=\"0.75\", markersize=2, linestyle=\"none\")\n",
    "    sns.boxplot(x=\"sample\", y=\"perc_identity\", hue=\"part\", data=data, flierprops=flierprops)\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.title(\"Boxplot of e-values for ABR and 16S parts across samples -Filtered-\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Percentage identity\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(filtered_fasta_files, sample_names, param, output_file):\n",
    "    \"\"\"Main function to process partitioned files for each sample and generate the plot.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Loop over each sample's partitioned CSV files\n",
    "    for sample in sample_names:\n",
    "        data = read_and_process_partitioned_data(\n",
    "            [file for file in filtered_fasta_files], sample, param\n",
    "        )\n",
    "        if data is not None:\n",
    "            all_data.append(data)\n",
    "\n",
    "    if all_data:\n",
    "        combined_data = pd.concat(all_data)\n",
    "        plot_boxplots(combined_data, output_file)\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "    result_dir = base / \".test_steps/results\"\n",
    "\n",
    "    filter_result = result_dir / \"filtered_result.csv\"\n",
    "    boxplot = result_dir / f\"combine_boxplot.png\"\n",
    "    \n",
    "    filtered_fasta_files = filter_result\n",
    "    output_file = boxplot  # Single output file for all panels\n",
    "    sample_names = \"test_epic_data\"\n",
    "    param = \"perc_identity\"\n",
    "    main([str(filtered_fasta_files)], [sample_names], param, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create Attrition plot\n",
    "# Selfwritten python scripts \"plot_attrition.py\"\n",
    "# Input: overview table\n",
    "# Output: plot of count overview throughout ERMA process with respect to rejection breakdown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pathlib\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "sample = \"test_epic_data\"\n",
    "result_dir = base / \".test_steps/results\"\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "overview_plot = result_dir / \"overview_plot.png\"\n",
    "\n",
    "# === Category Definitions (now match the final labels directly) ===\n",
    "MAIN_CATEGORIES = [\n",
    "    \"Number of FastQ input reads\",\n",
    "    \"Merged similarity hits\",\n",
    "    \"Filtered fusion reads\",\n",
    "]\n",
    "\n",
    "FILTER_REASONS = {\n",
    "    \"Diamond hits < similarity threshold\": \"royalblue\",\n",
    "    \"Diamond hits NOT highest percentage identity per query\": \"purple\",\n",
    "    \"Usearch hits < similarity threshold\": \"#a6d854\",\n",
    "    \"Usearch hits NOT highest percentage identity per query\": \"#66c2a5\",\n",
    "    \"Query hit in only one of two databases\": \"#ffd92f\",\n",
    "}\n",
    "\n",
    "MAIN_COLOR_MAP = {\n",
    "    \"Number of FastQ input reads\": \"seagreen\",\n",
    "    \"Merged similarity hits\": \"#fc8d62\",\n",
    "    \"Filtered fusion reads\": \"#8da0cb\",\n",
    "}\n",
    "\n",
    "# === Load and summarize the table ===\n",
    "def load_and_summarize_data(path):\n",
    "    df = pd.read_csv(path, names=[\"sample\", \"state\", \"count\"])\n",
    "    df[\"count\"] = df[\"count\"].astype(int).abs()\n",
    "\n",
    "    main_df = df[df[\"state\"].isin(MAIN_CATEGORIES)].pivot(index=\"sample\", columns=\"state\", values=\"count\").fillna(0)\n",
    "    filter_df = df[df[\"state\"].isin(FILTER_REASONS)].pivot(index=\"sample\", columns=\"state\", values=\"count\").fillna(0)\n",
    "\n",
    "    return main_df, filter_df\n",
    "\n",
    "# === Plotting function ===\n",
    "def plot_summary(main_df, filter_df, output_path):\n",
    "    samples = main_df.index\n",
    "    x = np.arange(len(samples))\n",
    "    bar_width = 0.18\n",
    "    overlay_width = 0.1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Plot main bars with offsets\n",
    "    offsets = np.linspace(-bar_width, bar_width, len(MAIN_CATEGORIES))\n",
    "    for i, col in enumerate(MAIN_CATEGORIES):\n",
    "        if col not in main_df.columns:\n",
    "            continue\n",
    "        ax.bar(\n",
    "            x + offsets[i],\n",
    "            main_df[col],\n",
    "            bar_width,\n",
    "            label=col,\n",
    "            color=MAIN_COLOR_MAP.get(col, \"gray\"),\n",
    "        )\n",
    "\n",
    "    # Plot filter stack bars *on top* of \"Filtered fusion reads\"\n",
    "    if \"Filtered fusion reads\" in main_df.columns:\n",
    "        bottom = main_df[\"Filtered fusion reads\"].values.copy()\n",
    "    else:\n",
    "        bottom = np.zeros_like(x)\n",
    "\n",
    "    for reason in FILTER_REASONS:\n",
    "        heights = filter_df[reason].values if reason in filter_df.columns else np.zeros_like(x)\n",
    "        ax.bar(\n",
    "            x + bar_width,\n",
    "            heights,\n",
    "            overlay_width,\n",
    "            bottom=bottom,\n",
    "            label=reason,\n",
    "            color=FILTER_REASONS.get(reason, \"gray\"),\n",
    "        )\n",
    "        bottom += heights\n",
    "\n",
    "    # Axis formatting\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(samples, rotation=45)\n",
    "    ax.set_ylabel(\"Similarity search hit count\")\n",
    "    ax.set_xlabel(\"Sample\")\n",
    "    ax.set_title(\"Similarity Search Processing with Rejection Breakdown\")\n",
    "\n",
    "    # Split legend into main vs. filter\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    main_labels = MAIN_CATEGORIES\n",
    "    filter_labels = FILTER_REASONS\n",
    "\n",
    "    legend1 = ax.legend(\n",
    "        [handles[labels.index(l)] for l in main_labels if l in labels],\n",
    "        main_labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        title=\"Hit Process\",\n",
    "    )\n",
    "    legend2 = ax.legend(\n",
    "        [handles[labels.index(l)] for l in filter_labels if l in labels],\n",
    "        filter_labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.02, 0.55),\n",
    "        title=\"Filtering Reasons\",\n",
    "    )\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "# === Execute ===\n",
    "main_df, filter_df = load_and_summarize_data(overview_table)\n",
    "plot_summary(main_df, filter_df, overview_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create Abundance data\n",
    "# Selfwritten python script \"single_genera_abundance_table.py\"\n",
    "# Input: Overview table created iteritavely within the snakemake run\n",
    "# Output: barplots for all samples showing generated and filtered similarity search hits\n",
    "# Note: Overview Table is created here after the process while in the original snakemake run\n",
    "#       it's created iteratively within the workflow.\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def write_dummy_line(sample_name):\n",
    "    dummy_line = {\n",
    "        \"sample\": sample_name,\n",
    "        \"AMR Gene Family\": \"NA\",\n",
    "        \"genus\": \"NA\",\n",
    "        \"genus_count\": 0,\n",
    "        \"total_count\": 0,\n",
    "        \"relative_genus_count\": 0,\n",
    "    }\n",
    "    merged_data = pd.DataFrame([dummy_line])\n",
    "    return merged_data\n",
    "\n",
    "def process_combined_data(combined_data, sample_name):\n",
    "\n",
    "    combined_data[\"sample\"] = sample_name\n",
    "\n",
    "    genus_counts = (\n",
    "        combined_data.groupby([\"sample\", \"AMR Gene Family\", \"genus\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"genus_count\")\n",
    "    )\n",
    "\n",
    "    total_counts = (\n",
    "        genus_counts.groupby([\"sample\", \"AMR Gene Family\"])[\"genus_count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_count\")\n",
    "    )\n",
    "\n",
    "    genus_counts = pd.merge(\n",
    "        genus_counts, total_counts, on=[\"sample\", \"AMR Gene Family\"], how=\"left\"\n",
    "    )\n",
    "    genus_counts[\"relative_genus_count\"] = round(\n",
    "        genus_counts[\"genus_count\"] / genus_counts[\"total_count\"], 4\n",
    "    )\n",
    "\n",
    "    return genus_counts\n",
    "\n",
    "\n",
    "def export_genera_abundance(input_files, sample_name, parts, output_path):\n",
    "    sample_input_files = [f for f in input_files]\n",
    "    part_dfs = []\n",
    "    for part in parts:\n",
    "        matching_files = [f for f in sample_input_files]\n",
    "        print(sample_input_files,matching_files)\n",
    "        if not matching_files:\n",
    "            continue\n",
    "        input_file = matching_files[0]\n",
    "        df = pd.read_csv(\n",
    "            input_file, sep=\",\",  header=0\n",
    "        )\n",
    "        part_dfs.append(df)\n",
    "\n",
    "    if not part_dfs:\n",
    "        print(f\"No valid parts found for sample: {sample_name}\")\n",
    "        dummy_df = write_dummy_line(sample_name)\n",
    "        dummy_df.to_csv(output_path, index=False)\n",
    "        return        \n",
    "\n",
    "    full_sample_df = pd.concat(part_dfs, ignore_index=True)\n",
    "    processed_data = process_combined_data(full_sample_df, sample_name)\n",
    "\n",
    "    processed_data = processed_data.sort_values(\n",
    "        by=[\"sample\", \"genus_count\"], ascending=False\n",
    "    )\n",
    "\n",
    "    display(processed_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base = pathlib.Path(os.path.dirname(pathlib.Path().resolve()))\n",
    "    result_dir = base / \".test_steps/results\"\n",
    "\n",
    "    filter_result = result_dir / \"filtered_result.csv\"\n",
    "    table = result_dir / f\"single_abundance_table.csv\"\n",
    "    \n",
    "    filtered_fasta_files = filter_result\n",
    "    \n",
    "    input_file = filter_result\n",
    "    output_path = table\n",
    "    sample_name = \"test_epic_data\"    \n",
    "    parts = [\"001\"]\n",
    "    export_genera_abundance([str(input_file)], sample_name, parts, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path().resolve()\n",
    "result_dir = base / \"results\"\n",
    "overview_table = result_dir / \"overview_table.txt\"\n",
    "overview_html = \"overview_table.html\"\n",
    "\n",
    "# Read the input table\n",
    "df = pd.read_csv(overview_table, sep=\",\", header=None, names=[\"sample\",\"step\",\"total_count\"])\n",
    "\n",
    "# Mapping step -> State\n",
    "step_to_state = {\n",
    "    \"Number of FastQ input reads\": \"Input reads\",\n",
    "    \"Diamond output hits\": \"Similarity search\",\n",
    "    \"Usearch output hits\": \"Similarity search\",\n",
    "    \"Merged similarity hits\": \"Similarity search\",\n",
    "    \"Diamond hits < similarity threshold\": \"Filtration\",\n",
    "    \"Diamond hits NOT highest percentage identity per query\": \"Filtration\",\n",
    "    \"Usearch hits < similarity threshold\": \"Filtration\",\n",
    "    \"Usearch hits NOT highest percentage identity per query\": \"Filtration\",\n",
    "    \"Query hit in only one of two databases\": \"Filtration\",\n",
    "    \"Filtered fusion reads\": \"Output reads\"\n",
    "}\n",
    "\n",
    "df[\"state\"] = df[\"step\"].map(step_to_state)\n",
    "\n",
    "# Reorder and sort\n",
    "df = df[[\"sample\", \"state\", \"step\", \"total_count\"]]\n",
    "state_order = [\"Input reads\", \"Similarity search\", \"Filtration\", \"Output reads\"]\n",
    "df[\"state\"] = pd.Categorical(df[\"state\"], categories=state_order, ordered=True)\n",
    "df = df.sort_values(by=[\"sample\", \"state\"])\n",
    "\n",
    "# === HTML with rowspan for merged cells ===\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table.styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 0.95em;\n",
    "        font-family: sans-serif;\n",
    "        min-width: 600px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    table.styled-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    table.styled-table th,\n",
    "    table.styled-table td {\n",
    "        padding: 10px 12px;\n",
    "        border: 1px solid #ddd;\n",
    "    }\n",
    "    table.styled-table tbody tr:nth-child(even) {\n",
    "        background-color: #f3f3f3;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table class=\"styled-table\">\n",
    "<thead>\n",
    "    <tr><th>Sample</th><th>State</th><th>Step</th><th>Count</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Group and track rowspans\n",
    "grouped = df.groupby([\"sample\", \"state\"])\n",
    "for (sample, state), group in grouped:\n",
    "    sample_rowspan = len(df[df[\"sample\"] == sample])\n",
    "    state_rowspan = len(group)\n",
    "    \n",
    "    first_state = True\n",
    "    for i, row in group.iterrows():\n",
    "        html += \"<tr>\"\n",
    "        if i == df[df[\"sample\"] == sample].index[0]:\n",
    "            html += f'<td rowspan=\"{sample_rowspan}\">{sample}</td>'\n",
    "        if first_state:\n",
    "            html += f'<td rowspan=\"{state_rowspan}\">{state}</td>'\n",
    "            first_state = False\n",
    "        html += f\"<td>{row['step']}</td><td>{row['total_count']}</td>\"\n",
    "        html += \"</tr>\"\n",
    "\n",
    "html += \"\"\"\n",
    "</tbody>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(html))\n",
    "# Write to file\n",
    "with open(overview_html, \"w\") as f:\n",
    "    f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# === Paths ===\n",
    "base = pathlib.Path().resolve()\n",
    "result_dir = base / \"results\"\n",
    "overview_table = result_dir / \"genera_abundance.csv\"\n",
    "overview_html = \"\"\n",
    "\n",
    "# Read the input table\n",
    "df = pd.read_csv(overview_table, sep=\",\", header=0)\n",
    "\n",
    "# === HTML with rowspan for merged cells ===\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table.styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 0.95em;\n",
    "        font-family: sans-serif;\n",
    "        min-width: 600px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    table.styled-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    table.styled-table th,\n",
    "    table.styled-table td {\n",
    "        padding: 10px 12px;\n",
    "        border: 1px solid #ddd;\n",
    "    }\n",
    "    table.styled-table tbody tr:nth-child(even) {\n",
    "        background-color: #f3f3f3;\n",
    "    }\n",
    "    table.styled-table tbody tr:hover {\n",
    "        background-color: #f1f1f1;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table class=\"styled-table\">\n",
    "<thead>\n",
    "    <tr><th>Sample</th><th>AMR Gene Family</th><th>Genus</th><th>Fusion Read Count</th><th>Relative</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Group and track rowspans\n",
    "grouped = df.groupby([\"sample\", \"AMR Gene Family\"])\n",
    "for (sample, family), group in grouped:\n",
    "    sample_rowspan = len(df[df[\"sample\"] == sample])\n",
    "    family_rowspan = len(group)\n",
    "    amr = df[(df[\"sample\"] == sample) & (df[\"AMR Gene Family\"] == family)]\n",
    "    reads_per_amr = amr[\"genus_count\"].sum()\n",
    "    amr_line = f\"{family}<br><span style='font-size: 0.85em'> Total Fusion Reads: {reads_per_amr}</span>\"\n",
    "    first_family = True\n",
    "    for i, row in group.iterrows():\n",
    "        html += \"<tr>\"\n",
    "        if i == df[df[\"sample\"] == sample].index[0]:\n",
    "            html += f'<td rowspan=\"{sample_rowspan}\">{sample}</td>'\n",
    "        if first_family:\n",
    "            html += f'<td rowspan=\"{family_rowspan}\">{amr_line}</td>'\n",
    "            first_family = False\n",
    "        html += f\"<td>{row['genus']}</td><td>{row['genus_count']}</td><td>{row['relative_genus_count']}</td>\"\n",
    "        html += \"</tr>\"\n",
    "\n",
    "html += \"\"\"\n",
    "</tbody>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(html))\n",
    "# Write to file\n",
    "#with open(overview_html, \"w\") as f:\n",
    "#    f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Root log directory (adjust if needed)\n",
    "log_dir = \"/local/work/adrian/ERMA/logs\"   # replace with your actual path\n",
    "output = \"/local/work/adrian/ERMA/logs/logs.json\"\n",
    "\n",
    "logs = {}\n",
    "\n",
    "for rule in sorted(os.listdir(log_dir)):\n",
    "    rule_path = os.path.join(log_dir, rule)\n",
    "    if not os.path.isdir(rule_path):\n",
    "        continue\n",
    "\n",
    "    rule_logs = {}\n",
    "    for log_file in sorted(os.listdir(rule_path)):\n",
    "        file_path = os.path.join(rule_path, log_file)\n",
    "\n",
    "        # Skip empty files\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "\n",
    "        # Use filename without extension as sample name\n",
    "        sample = os.path.splitext(log_file)[0]\n",
    "\n",
    "        # Read log text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        rule_logs[sample] = text\n",
    "\n",
    "    if rule_logs:  # only keep non-empty rules\n",
    "        logs[rule] = rule_logs\n",
    "\n",
    "# Write JSON\n",
    "with open(output, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(logs, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"logs.json written with\", sum(len(v) for v in logs.values()), \"log entries.\")\n",
    "\n",
    "# Remove all subfolders in the log directory\n",
    "for rule in os.listdir(log_dir):\n",
    "    rule_path = os.path.join(log_dir, rule)\n",
    "    if os.path.isdir(rule_path):\n",
    "        shutil.rmtree(rule_path)\n",
    "\n",
    "print(\"All subfolders removed, only logs.json remains.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "\n",
    "card_fasta = \"/local/work/adrian/ERMA/data/card_db/protein_fasta_protein_homolog_model.fasta\"\n",
    "output_fasta = \"/local/work/adrian/mge_databases/test_ERMA_carddb/protein_fasta_with_uniprot.fasta\"\n",
    "out_info = \"/local/work/adrian/mge_databases/test_ERMA_carddb/\"\n",
    "targets = [\"int1\"]\n",
    "cluster = \"100\"\n",
    "size = 500\n",
    "log = \"/local/work/adrian/mge_databases/test_ERMA_carddb/log.txt\"\n",
    "\n",
    "base_url = \"https://rest.uniprot.org/uniref/search\"\n",
    "headers = {\"accept\": \"text/tab-separated-values\"}\n",
    "identity_map = {\"100\": \"1.0\", \"90\": \"0.9\", \"50\": \"0.5\"}       \n",
    "\n",
    "def fetch_all(base_url, params, headers, max_entries):\n",
    "    \"\"\"Fetch pages from UniProt REST API with an optional total entry limit.\"\"\"\n",
    "    all_data = []\n",
    "    total_entries = 0\n",
    "\n",
    "    while True:\n",
    "        resp = requests.get(base_url, headers=headers, params=params)\n",
    "        resp.raise_for_status()\n",
    "        text = resp.text\n",
    "\n",
    "        # Count FASTA entries\n",
    "        n_entries = text.count(\">\")\n",
    "        total_entries += n_entries\n",
    "        all_data.append(text)\n",
    "\n",
    "        # Stop if limit reached\n",
    "        if max_entries and total_entries >= max_entries:\n",
    "            print(f\"Reached max_entries={max_entries}, stopping.\")\n",
    "            break\n",
    "\n",
    "        # Check for next page\n",
    "        next_link = resp.links.get(\"next\", {}).get(\"url\")\n",
    "        if not next_link:\n",
    "            break\n",
    "        base_url = next_link\n",
    "        params = {}\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return \"\".join(all_data)\n",
    "\n",
    "\n",
    "with open(log, \"w\") as lf, open(output_fasta, \"w\") as out:\n",
    "    lf.write(f\"Extending CARD with UniRef{cluster} sequences for targets: {targets}\\n\")\n",
    "\n",
    "    # Write original CARD sequences first\n",
    "    with open(card_fasta, \"r\") as cf:\n",
    "        out.write(cf.read())\n",
    "\n",
    "    for t in targets:\n",
    "        query = f\"{t} AND identity:{identity_map[cluster]}\"\n",
    "        params = {\n",
    "            \"query\": query, \n",
    "            \"fields\": \"id,cluster,identity,uniprot_id,organism_name,organism_id,length,member_count\",\n",
    "            \"format\": \"tsv\"}\n",
    "        lf.write(f\"Fetching UniRef{cluster} sequences for: {t}\\n\")\n",
    "        try:\n",
    "            fasta = fetch_all(base_url, params, headers,size)\n",
    "            out.write(fasta)\n",
    "        except Exception as e:\n",
    "            lf.write(f\"Failed to fetch {t}: {e}\\n\")\n",
    "        time.sleep(0.5)  # UniProt’s API enforces ~3 requests/sec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA sequences saved to: uniref100_int1.fasta\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "\n",
    "card_fasta = \"/local/work/adrian/ERMA/data/card_db/protein_fasta_protein_homolog_model.fasta\"\n",
    "output_tsv = \"/local/work/adrian/mge_databases/test_ERMA_carddb/protein_fasta_with_uniprot.tsv\"\n",
    "output_fasta = \"/local/work/adrian/mge_databases/test_ERMA_carddb/protein_fasta_with_uniprot.fasta\"\n",
    "out_info = \"/local/work/adrian/mge_databases/test_ERMA_carddb/\"\n",
    "targets = [\"int1\",\"inti1\"]\n",
    "cluster = \"100\"\n",
    "log = \"/local/work/adrian/mge_databases/test_ERMA_carddb/log.txt\"\n",
    "\n",
    "base_url = \"https://rest.uniprot.org/uniref/search\"\n",
    "headers = {\"accept\": \"text/plain\"}\n",
    "identity_map = {\"100\": \"1.0\", \"90\": \"0.9\", \"50\": \"0.5\"}       \n",
    "\n",
    "def fetch_all(base_url, params, headers, max_entries):\n",
    "    \"\"\"Fetch pages from UniProt REST API with an optional total entry limit.\"\"\"\n",
    "    all_data = []\n",
    "    total_entries = 0\n",
    "\n",
    "    while True:\n",
    "        resp = requests.get(base_url, headers=headers, params=params)\n",
    "        resp.raise_for_status()\n",
    "        text = resp.text\n",
    "\n",
    "        # Count FASTA entries\n",
    "        n_entries = text.count(\">\")\n",
    "        total_entries += n_entries\n",
    "        all_data.append(text)\n",
    "\n",
    "        # Stop if limit reached\n",
    "        if max_entries and total_entries >= max_entries:\n",
    "            print(f\"Reached max_entries={max_entries}, stopping.\")\n",
    "            break\n",
    "\n",
    "        # Check for next page\n",
    "        next_link = resp.links.get(\"next\", {}).get(\"url\")\n",
    "        if not next_link:\n",
    "            break\n",
    "        base_url = next_link\n",
    "        params = {}\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return \"\".join(all_data)\n",
    "\n",
    "def df_to_fasta(df):\n",
    "    \"\"\"Convert DataFrame to FASTA formatted string.\"\"\"\n",
    "    fasta_lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        header = f\">{row['Cluster ID']} {row['Common taxon']}\".strip()\n",
    "        seq = row[\"Reference sequence\"].replace(\" \", \"\")\n",
    "        fasta_lines.append(f\"{header}\\n{seq}\")\n",
    "    return \"\\n\".join(fasta_lines)\n",
    "\n",
    "df_final=[]\n",
    "for t in targets:\n",
    "    query = f\"{t} AND identity:1.0\"\n",
    "    params = {\"query\": query,\"fields\":[\"id\",\"name\",\"common_taxon\",\"identity\",\"sequence\"], \"format\": \"tsv\"}\n",
    "    fetch = fetch_all(base_url, params, headers,1000)\n",
    "    df = pd.read_csv(io.StringIO(fetch),header=0,sep='\\t')\n",
    "    df[\"Uniref query\"] = t\n",
    "    df[\"db\"] = \"Uniref\"\n",
    "    df_final.append(df)\n",
    "if df_final:\n",
    "    result = pd.concat(df_final,ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    result[\"Cluster Name\"] = result[\"Cluster Name\"].str.replace(\"Cluster:\",\"\")\n",
    "    result = result.drop(result[result[\"Common taxon\"] == \"Common taxon\"].index) # remove double headers\n",
    "    result.to_csv(output_tsv,index=False)\n",
    "    fasta_text = df_to_fasta(result)\n",
    "    with open(card_fasta, \"r\") as cf, open(output_fasta, \"w\") as out:\n",
    "        out.write(cf.read().strip() + \"\\n\" + fasta_text)\n",
    "\n",
    "    print(f\"Final combined file written: {output_fasta}\")\n",
    "else:\n",
    "    print(\"No data fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sys\n",
    "\n",
    "# ---------------------------- #\n",
    "# ---------- HELPERS ---------- #\n",
    "# ---------------------------- #\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def collapse_by_threshold(series, threshold, label):\n",
    "    \"\"\"Collapse low-frequency categories below threshold into a single label.\"\"\"\n",
    "    freq = series.value_counts(normalize=True)\n",
    "    rare = freq[freq < threshold].index\n",
    "    return series.replace(rare, label)\n",
    "\n",
    "\n",
    "def prepare_data(uniref_abundance, threshold):\n",
    "    \"\"\"Read, filter, merge and preprocess data for plotting.\"\"\"\n",
    "    len_dict={}\n",
    "    \n",
    "    df = pd.read_csv(uniref_abundance).rename(columns={\"subject_id_ABR\": \"Cluster ID\"})\n",
    "    \n",
    "    for query in df[\"Uniref query\"].unique():\n",
    "        count = df[df[\"Uniref query\"] == query]\n",
    "        len_dict[query]=count[\"genus_count\"].sum()\n",
    "    \n",
    "    df[\"genus\"] = df.groupby(\"Uniref query\", group_keys=False)[\"genus\"].apply(\n",
    "        collapse_by_threshold, threshold=threshold, label=\"Silva_lowfreq_bin\"\n",
    "    )\n",
    "    df[\"Common taxon\"] = df.groupby(\"Uniref query\", group_keys=False)[\"Common taxon\"].apply(\n",
    "        collapse_by_threshold, threshold=threshold, label=\"Uniref_lowfreq_bin\"\n",
    "    )\n",
    "    return df,len_dict\n",
    "\n",
    "def give_dummy_plot(output_html):\n",
    "    print(\"Here\")\n",
    "    fig = go.Figure()\n",
    "    fig.add_annotation(\n",
    "        text=\"No data available for plotting.<br>(Input files contained zero matching rows.)\",\n",
    "        x=0.5, y=0.5,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=18)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        height=300,\n",
    "        width=700,\n",
    "        title_text=\"Fusion Read Summary (No Data)\"\n",
    "    )\n",
    "    fig.write_html(output_html)\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_summary(uniref_df,len_dict,card_abundance, output_html):\n",
    "    \"\"\"Generate 3-row plot grid:\n",
    "    Row 1: Dummy pie chart\n",
    "    Row 2: Barplot of Cluster Name frequencies\n",
    "    Row 3: Sankey diagram (UniRef taxon ↔ SILVA genus)\n",
    "    \"\"\"\n",
    "    uniref_count = uniref_df[\"genus_count\"].sum()\n",
    "    card_df = pd.read_csv(card_abundance)\n",
    "    card_count = card_df[\"genus_count\"].sum()\n",
    "\n",
    "    queries = uniref_df[\"Uniref query\"].unique()\n",
    "    n_queries = len(queries)\n",
    "    if n_queries == 0:\n",
    "        give_dummy_plot(output_html)\n",
    "        return\n",
    "\n",
    "    \n",
    "    row_titles = [\n",
    "        \"Fusion read composition by database hit\",\n",
    "        \"Cluster Name frequency distribution\",\n",
    "        \"Stacked barplot\",\n",
    "        \"UniRef taxon ↔ SILVA genus mapping\",\n",
    "    ]\n",
    "\n",
    "    subplot_titles = [\n",
    "        f\"{q}:{row_titles[r]}\"\n",
    "        for r in range(len(row_titles))\n",
    "        for q in queries\n",
    "    ]\n",
    "\n",
    "    # Three rows: Pie → Barplot → Sankey\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=n_queries,\n",
    "        subplot_titles=subplot_titles,\n",
    "        specs=[\n",
    "            [{\"type\": \"domain\"}] * n_queries,   # Row 1: pie chart\n",
    "            [{\"type\": \"xy\"}] * n_queries,       # Row 2: barplot\n",
    "            [{\"type\": \"xy\"}] * n_queries,      # Row 3: Stacked barplot            \n",
    "            [{\"type\": \"sankey\"}] * n_queries    # Row 4: sankey\n",
    "        ],\n",
    "        horizontal_spacing=0.1,\n",
    "        vertical_spacing=0.05,\n",
    "    )\n",
    "\n",
    "    # --- Loop per UniRef query column\n",
    "    for i, q in enumerate(queries, start=1):\n",
    "        df_q = uniref_df[uniref_df[\"Uniref query\"] == q].copy()\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # ROW 1: Pie chart (two dummy values)\n",
    "        # ------------------------------------------------------\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=[\"CARD Hits\", \"Uniref Hits\"],\n",
    "                values=[card_count, len_dict[q]],                            \n",
    "                textinfo=\"label+percent\",                \n",
    "                hoverinfo=\"label+percent\",                \n",
    "                hole=0.3\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # ROW 2: Barplot of Cluster Name frequencies\n",
    "        # ------------------------------------------------------\n",
    "        freq = df_q[\"Cluster Name\"].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=freq.values,\n",
    "                y=freq.index,\n",
    "                orientation=\"h\",\n",
    "                marker=dict(opacity=0.75),\n",
    "                name=f\"{q} frequencies\"\n",
    "            ),\n",
    "            row=2, col=i\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            automargin=True,\n",
    "            row=2, col=i\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # ROW 3: Genus abundance\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        genus_counts = (\n",
    "            df_q[\"genus\"].value_counts(normalize=True)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"genus\", \"genus\": \"relative_abundance\"})\n",
    "        )\n",
    "        for _, row in genus_counts.iterrows():\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[q],\n",
    "                    y=[row[\"relative_abundance\"]],\n",
    "                    name=row[\"genus\"],\n",
    "                    width=0.2,\n",
    "                ),\n",
    "                row=3, col=i\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            barmode=\"stack\")\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # ROW 4: Sankey diagram\n",
    "        # ------------------------------------------------------\n",
    "        genus_labels = list(df_q[\"genus\"].unique())\n",
    "        taxon_labels = list(df_q[\"Common taxon\"].unique())\n",
    "        all_labels = genus_labels + taxon_labels\n",
    "        idx = {k: v for v, k in enumerate(all_labels)}\n",
    "\n",
    "        sankey_links = {\n",
    "            \"source\": [idx[t] for t in df_q[\"Common taxon\"]],\n",
    "            \"target\": [idx[g] for g in df_q[\"genus\"]],\n",
    "            \"value\": [1] * len(df_q)\n",
    "        }\n",
    "\n",
    "        sankey = go.Sankey(\n",
    "            node=dict(label=all_labels, pad=10, thickness=12),\n",
    "            link=sankey_links\n",
    "        )\n",
    "        fig.add_trace(sankey, row=4, col=i)\n",
    "\n",
    "    # Layout ----------------------------------------------------\n",
    "    fig.update_layout(\n",
    "        height=1700,\n",
    "        width=450 * n_queries,\n",
    "        title_text=f\"Fusion Read Summary of non-CARD targets: {', '.join(queries)}\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    for ann in fig['layout']['annotations']:\n",
    "        ann['yshift'] = 10          # move title upward (increase for more space)\n",
    "        ann['font'] = dict(size=14) # optional: adjust font size\n",
    "\n",
    "    col_titles = queries  # e.g., [\"int1\", \"inti1\"]\n",
    "\n",
    "    fig.show()\n",
    "    #fig.write_html(output_html)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point for Snakemake execution.\"\"\"\n",
    "    card_abundance = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    uniref_abundance = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance_uniref.csv\"\n",
    "    output_html = \"plot.html\"\n",
    "\n",
    "    threshold = 0.01\n",
    "\n",
    "    uniref_df,len_dict = prepare_data(uniref_abundance, threshold)\n",
    "    plot_summary(uniref_df,len_dict,card_abundance, output_html)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Adrian Dörr.\n",
    "# Licensed under the MIT License (https://opensource.org/license/mit)\n",
    "# This file may not be copied, modified, or distributed\n",
    "# except according to those terms.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "This script takes the combined genus abundance table as input and counts\n",
    "the number of hits in relation to respective AMR families.\n",
    "\"\"\"\n",
    "\n",
    "# === HTML with rowspan for merged cells ===\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table.styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 0.95em;\n",
    "        font-query: sans-serif;\n",
    "        min-width: 600px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    table.styled-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    table.styled-table th,\n",
    "    table.styled-table td {\n",
    "        padding: 10px 12px;\n",
    "        border: 1px solid #ddd;\n",
    "    }\n",
    "    table.styled-table tbody tr:nth-child(even) {\n",
    "        background-color: #f3f3f3;\n",
    "    }\n",
    "    table.styled-table tbody tr:hover {\n",
    "        background-color: #f1f1f1;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table class=\"styled-table\">\n",
    "<thead>\n",
    "    <tr><th>Sample</th><th>Uniref query</th><th>Genus</th><th>Fusion Read Count</th><th>Relative</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_abundance_data(input_file, html, output_html):\n",
    "    df = pd.read_csv(input_file)\n",
    "    grouped_sample = df.groupby(\"sample\")\n",
    "    for sample, df_sample in grouped_sample:\n",
    "        grouped_query = df_sample.groupby(\"Uniref query\")\n",
    "        for query, df_query in grouped_query:\n",
    "            query_rowspan = len(df_query)\n",
    "            query_first_row = True\n",
    "            total_reads = df_query[\"genus_count\"].sum()\n",
    "            target_line = (\n",
    "                f\"{query}<br><span style='font-size:0.85em'>\"\n",
    "                f\"Total Fusion Reads: {total_reads}</span>\"\n",
    "            )\n",
    "            for i, row in df_query.iterrows():\n",
    "                html += \"<tr>\"\n",
    "                # Sample column appears ONCE per query\n",
    "                if query_first_row:\n",
    "                    html += f'<td rowspan=\"{query_rowspan}\">{sample}</td>'\n",
    "                    html += f'<td rowspan=\"{query_rowspan}\">{target_line}</td>'\n",
    "                    query_first_row = False\n",
    "                # Per-row data\n",
    "                html += (\n",
    "                    f\"<td>{row['genus']}</td>\"\n",
    "                    f\"<td>{row['genus_count']}</td>\"\n",
    "                    f\"<td>{row['relative_genus_count']}</td>\"\n",
    "                )\n",
    "                html += \"</tr>\"\n",
    "\n",
    "    html += \"\"\"\n",
    "    </tbody>\n",
    "    </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"plot.html\"\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    plot_abundance_data(input_file, html, output_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>AMR Gene Family</th>\n",
       "      <th>perc_identity_ABR</th>\n",
       "      <th>align_length_ABR</th>\n",
       "      <th>evalue_ABR</th>\n",
       "      <th>Drug Class</th>\n",
       "      <th>ARO Name</th>\n",
       "      <th>subject_id_ABR</th>\n",
       "      <th>db</th>\n",
       "      <th>subject_id_16S</th>\n",
       "      <th>genus</th>\n",
       "      <th>perc_identity_16S</th>\n",
       "      <th>align_length_16S</th>\n",
       "      <th>evalue_16S</th>\n",
       "      <th>sample</th>\n",
       "      <th>Common taxon</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Cluster Name</th>\n",
       "      <th>Uniref query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERR2399574.100031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>1.460000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_D5LHE1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>CP001616.3039747.3041280 Bacteria;Pseudomonado...</td>\n",
       "      <td>Tolumonas</td>\n",
       "      <td>98.3</td>\n",
       "      <td>289</td>\n",
       "      <td>1.100000e-141</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERR2399574.10004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>1.990000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_D5LHE1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>GU356331.1.1390 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>97.2</td>\n",
       "      <td>289</td>\n",
       "      <td>1.100000e-136</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ERR2399574.100229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.9</td>\n",
       "      <td>73</td>\n",
       "      <td>1.030000e-45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_D5LHE1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>AY945914.1.1502 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Rivicola</td>\n",
       "      <td>99.0</td>\n",
       "      <td>292</td>\n",
       "      <td>1.100000e-146</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERR2399574.100280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>1.990000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_D5LHE1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>GU356346.1.1398 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>99.3</td>\n",
       "      <td>290</td>\n",
       "      <td>3.100000e-142</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ERR2399574.100313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>2.070000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_D5LHE1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>HQ749869.1.1452 Bacteria;Bacillota;Clostridia;...</td>\n",
       "      <td>Roseburia</td>\n",
       "      <td>99.3</td>\n",
       "      <td>289</td>\n",
       "      <td>1.100000e-146</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>ERR2399574.50494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>2.500000e-44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_A0A4P8G675</td>\n",
       "      <td>uniref</td>\n",
       "      <td>JN104394.1.1424 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Rivicola</td>\n",
       "      <td>99.7</td>\n",
       "      <td>289</td>\n",
       "      <td>2.400000e-148</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IntI1</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>ERR2399574.52392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>4.210000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_UPI0029649DCC</td>\n",
       "      <td>uniref</td>\n",
       "      <td>GU356346.1.1398 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>97.6</td>\n",
       "      <td>290</td>\n",
       "      <td>6.800000e-134</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Vibrio sp. 1457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>class 1 integron integrase IntI1</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>ERR2399574.61860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>2.870000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_UPI0029649DCC</td>\n",
       "      <td>uniref</td>\n",
       "      <td>HQ663903.1.1539 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>96.9</td>\n",
       "      <td>289</td>\n",
       "      <td>5.300000e-135</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Vibrio sp. 1457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>class 1 integron integrase IntI1</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>ERR2399574.7939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.9</td>\n",
       "      <td>73</td>\n",
       "      <td>3.060000e-44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_UPI0029649DCC</td>\n",
       "      <td>uniref</td>\n",
       "      <td>AB745414.1.1511 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Tolumonas</td>\n",
       "      <td>99.3</td>\n",
       "      <td>279</td>\n",
       "      <td>4.000000e-141</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Vibrio sp. 1457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>class 1 integron integrase IntI1</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>ERR2399574.79923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.6</td>\n",
       "      <td>73</td>\n",
       "      <td>4.980000e-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UniRef100_UPI001363A904</td>\n",
       "      <td>uniref</td>\n",
       "      <td>GU356331.1.1390 Bacteria;Pseudomonadota;Gammap...</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>95.5</td>\n",
       "      <td>289</td>\n",
       "      <td>2.500000e-128</td>\n",
       "      <td>test_sample</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>1.0</td>\n",
       "      <td>class 1 integron integrase IntI1</td>\n",
       "      <td>inti1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1921 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               query_id AMR Gene Family  perc_identity_ABR  align_length_ABR  \\\n",
       "0     ERR2399574.100031             NaN               98.6                73   \n",
       "1      ERR2399574.10004             NaN               98.6                73   \n",
       "2     ERR2399574.100229             NaN               95.9                73   \n",
       "3     ERR2399574.100280             NaN               98.6                73   \n",
       "4     ERR2399574.100313             NaN               98.6                73   \n",
       "...                 ...             ...                ...               ...   \n",
       "1916   ERR2399574.50494             NaN               98.6                73   \n",
       "1917   ERR2399574.52392             NaN               98.6                73   \n",
       "1918   ERR2399574.61860             NaN               98.6                73   \n",
       "1919    ERR2399574.7939             NaN               95.9                73   \n",
       "1920   ERR2399574.79923             NaN               98.6                73   \n",
       "\n",
       "        evalue_ABR Drug Class ARO Name           subject_id_ABR      db  \\\n",
       "0     1.460000e-47        NaN      NaN         UniRef100_D5LHE1  uniref   \n",
       "1     1.990000e-47        NaN      NaN         UniRef100_D5LHE1  uniref   \n",
       "2     1.030000e-45        NaN      NaN         UniRef100_D5LHE1  uniref   \n",
       "3     1.990000e-47        NaN      NaN         UniRef100_D5LHE1  uniref   \n",
       "4     2.070000e-47        NaN      NaN         UniRef100_D5LHE1  uniref   \n",
       "...            ...        ...      ...                      ...     ...   \n",
       "1916  2.500000e-44        NaN      NaN     UniRef100_A0A4P8G675  uniref   \n",
       "1917  4.210000e-47        NaN      NaN  UniRef100_UPI0029649DCC  uniref   \n",
       "1918  2.870000e-47        NaN      NaN  UniRef100_UPI0029649DCC  uniref   \n",
       "1919  3.060000e-44        NaN      NaN  UniRef100_UPI0029649DCC  uniref   \n",
       "1920  4.980000e-47        NaN      NaN  UniRef100_UPI001363A904  uniref   \n",
       "\n",
       "                                         subject_id_16S      genus  \\\n",
       "0     CP001616.3039747.3041280 Bacteria;Pseudomonado...  Tolumonas   \n",
       "1     GU356331.1.1390 Bacteria;Pseudomonadota;Gammap...  Aeromonas   \n",
       "2     AY945914.1.1502 Bacteria;Pseudomonadota;Gammap...   Rivicola   \n",
       "3     GU356346.1.1398 Bacteria;Pseudomonadota;Gammap...  Aeromonas   \n",
       "4     HQ749869.1.1452 Bacteria;Bacillota;Clostridia;...  Roseburia   \n",
       "...                                                 ...        ...   \n",
       "1916  JN104394.1.1424 Bacteria;Pseudomonadota;Gammap...   Rivicola   \n",
       "1917  GU356346.1.1398 Bacteria;Pseudomonadota;Gammap...  Aeromonas   \n",
       "1918  HQ663903.1.1539 Bacteria;Pseudomonadota;Gammap...  Aeromonas   \n",
       "1919  AB745414.1.1511 Bacteria;Pseudomonadota;Gammap...  Tolumonas   \n",
       "1920  GU356331.1.1390 Bacteria;Pseudomonadota;Gammap...  Aeromonas   \n",
       "\n",
       "      perc_identity_16S  align_length_16S     evalue_16S       sample  \\\n",
       "0                  98.3               289  1.100000e-141  test_sample   \n",
       "1                  97.2               289  1.100000e-136  test_sample   \n",
       "2                  99.0               292  1.100000e-146  test_sample   \n",
       "3                  99.3               290  3.100000e-142  test_sample   \n",
       "4                  99.3               289  1.100000e-146  test_sample   \n",
       "...                 ...               ...            ...          ...   \n",
       "1916               99.7               289  2.400000e-148  test_sample   \n",
       "1917               97.6               290  6.800000e-134  test_sample   \n",
       "1918               96.9               289  5.300000e-135  test_sample   \n",
       "1919               99.3               279  4.000000e-141  test_sample   \n",
       "1920               95.5               289  2.500000e-128  test_sample   \n",
       "\n",
       "               Common taxon  Identity                         Cluster Name  \\\n",
       "0     Klebsiella pneumoniae       1.0   Class 1 integrase IntI1 (Fragment)   \n",
       "1     Klebsiella pneumoniae       1.0   Class 1 integrase IntI1 (Fragment)   \n",
       "2     Klebsiella pneumoniae       1.0   Class 1 integrase IntI1 (Fragment)   \n",
       "3     Klebsiella pneumoniae       1.0   Class 1 integrase IntI1 (Fragment)   \n",
       "4     Klebsiella pneumoniae       1.0   Class 1 integrase IntI1 (Fragment)   \n",
       "...                     ...       ...                                  ...   \n",
       "1916       Escherichia coli       1.0                                IntI1   \n",
       "1917        Vibrio sp. 1457       1.0     class 1 integron integrase IntI1   \n",
       "1918        Vibrio sp. 1457       1.0     class 1 integron integrase IntI1   \n",
       "1919        Vibrio sp. 1457       1.0     class 1 integron integrase IntI1   \n",
       "1920       Escherichia coli       1.0     class 1 integron integrase IntI1   \n",
       "\n",
       "     Uniref query  \n",
       "0           inti1  \n",
       "1           inti1  \n",
       "2           inti1  \n",
       "3           inti1  \n",
       "4           inti1  \n",
       "...           ...  \n",
       "1916        inti1  \n",
       "1917        inti1  \n",
       "1918        inti1  \n",
       "1919        inti1  \n",
       "1920        inti1  \n",
       "\n",
       "[1921 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>Uniref query</th>\n",
       "      <th>genus</th>\n",
       "      <th>Common taxon</th>\n",
       "      <th>Cluster Name</th>\n",
       "      <th>db</th>\n",
       "      <th>genus_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>relative_genus_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>698</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.3637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Tolumonas</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>254</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Arcobacter</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>162</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Pseudaeromonas</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>101</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Incertae Sedis</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>93</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Veillonella</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Vitreoscilla</td>\n",
       "      <td>Pseudomonadota</td>\n",
       "      <td>IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Vitreoscilla</td>\n",
       "      <td>Salmonella typhimurium</td>\n",
       "      <td>Truncated IntI1 DNA integrase</td>\n",
       "      <td>uniref</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Vitreoscilla</td>\n",
       "      <td>Thauera sp. 2A1</td>\n",
       "      <td>class 1 integron integrase IntI1</td>\n",
       "      <td>uniref</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>test_sample</td>\n",
       "      <td>inti1</td>\n",
       "      <td>Zoogloea</td>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>Class 1 integrase IntI1 (Fragment)</td>\n",
       "      <td>uniref</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sample Uniref query           genus            Common taxon  \\\n",
       "12   test_sample        inti1       Aeromonas   Klebsiella pneumoniae   \n",
       "133  test_sample        inti1       Tolumonas   Klebsiella pneumoniae   \n",
       "27   test_sample        inti1      Arcobacter   Klebsiella pneumoniae   \n",
       "102  test_sample        inti1  Pseudaeromonas   Klebsiella pneumoniae   \n",
       "70   test_sample        inti1  Incertae Sedis   Klebsiella pneumoniae   \n",
       "..           ...          ...             ...                     ...   \n",
       "142  test_sample        inti1     Veillonella   Klebsiella pneumoniae   \n",
       "144  test_sample        inti1    Vitreoscilla          Pseudomonadota   \n",
       "145  test_sample        inti1    Vitreoscilla  Salmonella typhimurium   \n",
       "146  test_sample        inti1    Vitreoscilla         Thauera sp. 2A1   \n",
       "148  test_sample        inti1        Zoogloea   Klebsiella pneumoniae   \n",
       "\n",
       "                            Cluster Name      db  genus_count  total_count  \\\n",
       "12    Class 1 integrase IntI1 (Fragment)  uniref          698         1919   \n",
       "133   Class 1 integrase IntI1 (Fragment)  uniref          254         1919   \n",
       "27    Class 1 integrase IntI1 (Fragment)  uniref          162         1919   \n",
       "102   Class 1 integrase IntI1 (Fragment)  uniref          101         1919   \n",
       "70    Class 1 integrase IntI1 (Fragment)  uniref           93         1919   \n",
       "..                                   ...     ...          ...          ...   \n",
       "142   Class 1 integrase IntI1 (Fragment)  uniref            1         1919   \n",
       "144                     IntI1 (Fragment)  uniref            1         1919   \n",
       "145        Truncated IntI1 DNA integrase  uniref            1         1919   \n",
       "146     class 1 integron integrase IntI1  uniref            1         1919   \n",
       "148   Class 1 integrase IntI1 (Fragment)  uniref            1         1919   \n",
       "\n",
       "     relative_genus_count  \n",
       "12                 0.3637  \n",
       "133                0.1324  \n",
       "27                 0.0844  \n",
       "102                0.0526  \n",
       "70                 0.0485  \n",
       "..                    ...  \n",
       "142                0.0005  \n",
       "144                0.0005  \n",
       "145                0.0005  \n",
       "146                0.0005  \n",
       "148                0.0005  \n",
       "\n",
       "[149 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copyright 2024 Adrian Dörr.\n",
    "# Licensed under the MIT License (https://opensource.org/license/mit)\n",
    "# This file may not be copied, modified, or distributed\n",
    "# except according to those terms.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "\"\"\"\n",
    "This script processes epicPCR data (ABR + 16S) combined over all samples and all parts\n",
    "to compute genus-level total and relative abundance per AMR Gene Family in a table.\n",
    "This table is later used to create the main result bubble plot.\n",
    "\"\"\"\n",
    "\n",
    "# Necessary columns to load in each dataframe\n",
    "\n",
    "\n",
    "def write_dummy_line(sample_name):\n",
    "    \"\"\"Create a dummy row for missing input and returns placeholder\"\"\"\n",
    "    dummy_line = {\n",
    "        \"sample\": sample_name,\n",
    "        \"AMR Gene Family\": \"NA\",\n",
    "        \"Drug Class\": \"NA\",\n",
    "        \"ARO Name\": \"NA\",\n",
    "        \"genus\": \"NA\",\n",
    "        \"genus_count\": 0,\n",
    "        \"total_count\": 0,\n",
    "        \"relative_genus_count\": 0,\n",
    "    }\n",
    "    return pd.DataFrame([dummy_line])\n",
    "\n",
    "\n",
    "def process_combined_data(df, info_df, sample_name):\n",
    "    df[\"sample\"] = sample_name\n",
    "    combined_data = df.merge(info_df, on=[\"subject_id_ABR\"])\n",
    "    display(combined_data)\n",
    "    # Count genus occurrences per AMR Gene Family\n",
    "    genus_counts = (\n",
    "        combined_data.groupby(\n",
    "            [\n",
    "                \"sample\",\n",
    "                \"Uniref query\",\n",
    "                \"genus\",\n",
    "                \"Common taxon\",\n",
    "                \"Cluster Name\",\n",
    "                \"db\",\n",
    "            ]\n",
    "        )\n",
    "        .size()\n",
    "        .reset_index(name=\"genus_count\")\n",
    "    )\n",
    "\n",
    "    # Calculate total genus count per AMR Gene Family within each sample\n",
    "    total_counts = (\n",
    "        genus_counts.groupby([\"sample\", \"Uniref query\"])[\"genus_count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_count\")\n",
    "    )\n",
    "\n",
    "    # Join and calculate relative abundance\n",
    "    result = pd.merge(genus_counts, total_counts, on=[\"sample\", \"Uniref query\"])\n",
    "    result[\"relative_genus_count\"] = round(\n",
    "        result[\"genus_count\"] / result[\"total_count\"], 4\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_and_merge_parts(file_list):\n",
    "    \"\"\"Load and merges dataframes over all samples\"\"\"\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file due to read error [{file}]: {repr(e)}\")\n",
    "    if data_frames:\n",
    "        merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        merged_df = pd.DataFrame()\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def export_genera_abundance(input_files, info_file, output_path):\n",
    "    \"\"\"Group input files by sample\"\"\"\n",
    "    sample_to_files = {}\n",
    "    for file in input_files:\n",
    "        # Extract sample name from the file path, assuming 3rd-to-last split is the sample name\n",
    "        sample = \"test_sample\"\n",
    "        sample_to_files.setdefault(sample, []).append(file)\n",
    "\n",
    "    all_data = []\n",
    "    info_df = pd.read_csv(info_file,header=0)\n",
    "    info_df = info_df.drop(['Reference sequence'],axis=1)\n",
    "    info_df = info_df.rename(columns={\"Cluster ID\": \"subject_id_ABR\"})\n",
    "\n",
    "    for sample_name, files in sample_to_files.items():\n",
    "        merged_data = load_and_merge_parts(files)\n",
    "        sample_data = process_combined_data(merged_data, info_df, sample_name)\n",
    "        all_data.append(sample_data)\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Export the final aggregated data to a CSV file\n",
    "    final_df= final_df.sort_values(\n",
    "        by=[\"sample\",\"genus_count\"], ascending=False\n",
    "    )\n",
    "    display(final_df)\n",
    "    #merged.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\"/local/work/adrian/ERMA/.test_steps/results/filtered_result.csv\"]\n",
    "    info_file = \"/local/work/adrian/mge_databases/test_ERMA_carddb/protein_fasta_with_uniprot.tsv\"\n",
    "    output_path = \"/local/work/adrian/ERMA/.test_steps/results/combined_genus_abundance_uniref.csv\"\n",
    "    #sys.stderr = open(snakemake.log[0], \"w\")\n",
    "    export_genera_abundance(input_files,info_file, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb",
   "language": "python",
   "name": "ipynb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
