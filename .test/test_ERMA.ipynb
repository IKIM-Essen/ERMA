{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conda environment with necessary packages\n",
    "# conda env create -f ERMA.yml (may take a while, use mamba if possible)\n",
    "# set ipynb kernel to ERMA\n",
    "# execute cells in order\n",
    "\n",
    "\n",
    "# Move test file to execute folder\n",
    "!cp data/test_epic_data.fastq.gz ../data/fastq/\n",
    "# Start snakemake pipeline measuring process time\n",
    "!time snakemake --cores all --use-conda --snakefile ../workflow/Snakefile --directory ../ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src= \"ERMA_workflow.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Step similarity search\n",
    "# Input: fasta files\n",
    "# Output: tabular similarity search results\n",
    "# commands:\n",
    "# usearch vs silva_v138.2 database (778 MB unzipped): usearch -usearch_local {input.fasta} -db {input.silva} -blast6out {output.silva_results} -evalue 1e-5 -threads {params.internal_threads} -strand plus -mincols 200 2> {log}\n",
    "# blastx vs card_v3.3.0 database (4.8 MB unzipped): blastx -query {input.fasta} -db {params.db}/card_db -out {output.card_results} -outfmt 6 -evalue 1e-5 -num_threads {params.internal_threads} 2> {log}\n",
    "\n",
    "import os,pathlib,pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "path = os.path.dirname(pathlib.Path().resolve())\n",
    "\n",
    "silva_result = os.path.join(path,\"results\",\"test_epic_data\",\"001\",\"SILVA_results.txt.gz\")\n",
    "card_result = os.path.join(path,\"results\",\"test_epic_data\",\"001\",\"card_results.txt.gz\")\n",
    "string1 = \"blastx -query {input.fasta} -db {params.db}/card_db -out {output.card_results} -outfmt 6 -evalue 1e-5 -num_threads {params.internal_threads} 2> {log}\"\n",
    "string2 = \"usearch -usearch_local {input.fasta} -db {input.silva} -blast6out {output.silva_results} -evalue 1e-5 -threads {params.internal_threads} -strand plus -mincols 200 2> {log}\"\n",
    "\n",
    "blast_columns = [\"query_id\", \"subject_id\", \"perc_identity\", \"align_length\", \"mismatches\",\"gap_opens\", \"q_start\", \"q_end\", \"s_start\", \"s_end\", \"evalue\", \"bit_score\"]\n",
    "\n",
    "\n",
    "for i in [[card_result,\"Card Similarity Search Result (BLASTX)\",string1],[silva_result,\"Silva Similarity Search Result (usearch)\",string2]]:\n",
    "    df = pd.read_csv(i[0],names=blast_columns,compression='gzip',sep='\\t')\n",
    "    df = df[df['evalue'] != 0]\n",
    "    summary = df.agg({\"perc_identity\": [\"mean\", \"min\", \"max\"],\"align_length\": [\"mean\", \"min\", \"max\"],\"evalue\": [\"mean\", \"min\", \"max\"]}).style.format({\"perc_identity\": \"{:.2f}\",\"align_length\": \"{:.2f}\",\"evalue\": \"{:.2e}\"})\n",
    "    display(Markdown(f\"## {i[1]}\"))\n",
    "    display(Markdown(f\" ```{i[2]}```\"))\n",
    "    display(Markdown(f\" Shape: {df.shape}\"))\n",
    "    display(summary)    \n",
    "    display(df.head(2))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Integrate similarity search results\n",
    "# Selfwritten python script \"integrate_blast_data.py\"\n",
    "# Input: blastx, usearch results, ARO Mapping file\n",
    "# Output: Processed integrated search results\n",
    "\n",
    "aro_mapping = os.path.join(path, \"data\", \"card_db\", \"aro_index.tsv\")\n",
    "aro_df = pd.read_csv(aro_mapping, sep=\"\\t\")\n",
    "\n",
    "int_result = os.path.join(path, \"results\", \"test_epic_data\", \"001\",\"integrated_filtered_results.csv\")\n",
    "int_df = pd.read_csv(int_result, sep=\",\", low_memory=False)\n",
    "int_df = int_df[int_df['evalue'] != 0]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Adding information to similarity results and integrate\n",
    "---\n",
    "## Mapping input: ARO File\n",
    "- Shape: {aro_df.shape}\\n\n",
    "- Mapping to CARD results: ```df.merge(aro_df, on='ARO Accession', how='left')```\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "display(aro_df[aro_df[\"AMR Gene Family\"] == \"OXA beta-lactamase;OXA-48-like beta-lactamase\"].head(2))\n",
    "\n",
    "rows_16s = len(int_df[int_df[\"part\"] == '16S'])\n",
    "rows_abr = len(int_df[int_df[\"part\"] == 'ABR'])\n",
    "orientation_counts = int_df['orientation'].value_counts().to_dict()\n",
    "subject_id_16 = int_df[int_df[\"part\"] == \"16S\"][\"subject_id\"].iloc[0]\n",
    "subject_id_abr = int_df[int_df[\"part\"] == \"ABR\"][\"subject_id\"].iloc[0]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Additional Information Added\n",
    "---\n",
    "### 1. Adding field for discriminating 16S and ABR\n",
    "Number of rows containing 16S information: **{rows_16s}**\\n\n",
    "Number of rows containing ABR information: **{rows_abr}**\n",
    "\n",
    "### 2. Adding column for relevant information of similarity search \"subject id\"\n",
    "genus = **{subject_id_16.split(\";\")[-2]}** <--- {subject_id_16}\\n\n",
    "ARO Accession = **{subject_id_abr.split(\"|\")[-2]}** <--- {subject_id_abr}\n",
    "### 3. Merge 16S and ABR results to one table (integrated_results.csv)\n",
    "```combined_df = pd.concat([silva_df,card_df])```\\n\n",
    "\"\"\"))\n",
    "\n",
    "part_16S = int_df[int_df[\"part\"] == '16S'].head(1)\n",
    "part_ABR = int_df[int_df[\"part\"] == 'ABR'].head(1)\n",
    "summary = int_df.agg({\"perc_identity\": [\"mean\", \"min\", \"max\"],\"align_length\": [\"mean\", \"min\", \"max\"],\"evalue\": [\"mean\", \"min\", \"max\"]}).style.format({\"perc_identity\": \"{:.2f}\",\"align_length\": \"{:.0f}\",\"evalue\": \"{:.2e}\"})\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Integrated Results\n",
    "- **Shape**: {int_df.shape}\n",
    "---\n",
    "### Summary Statistics\n",
    "\"\"\"))\n",
    "\n",
    "display(summary)\n",
    "merged_df = pd.concat([part_16S, part_ABR])\n",
    "columns_of_interest = ['query_id', 'subject_id', 'perc_identity', 'align_length','evalue','bit_score', 'part', 'primaryAccession','genus', 'ARO Accession','AMR Gene Family', 'Drug Class', 'Resistance Mechanism','CARD Short Name']\n",
    "display(merged_df[columns_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filter Blast results\n",
    "# Selfwritten python script \"filter_blast_results.py\"\n",
    "# Input: integrated_filtered_results.csv\n",
    "# Output: filtered_results.csv\n",
    "\n",
    "def process_orientation_and_counts(group):\n",
    "    max_perc_identity = group[\"perc_identity\"].max()\n",
    "    filtered_group = group[group[\"perc_identity\"] == max_perc_identity]\n",
    "    return filtered_group\n",
    "\n",
    "minsim = int_df[int_df['perc_identity'] > 80.0]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Filtering Steps\n",
    "---\n",
    "### 1. Filter by minimal similarity (default 0.8)\\n\n",
    "- ```f_df = df[df['perc_identity'] > 80.0]```\\n\n",
    "- Integrated dataframe before reduction: {int_df.shape[0]} vs after reduction **{minsim.shape[0]}** (reduction of {int_df.shape[0]-minsim.shape[0]})\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "abr_data = minsim[(minsim['part'] == 'ABR')]\n",
    "abr_before = abr_data.shape[0]\n",
    "abr_data = abr_data.groupby(\"query_id\").apply(process_orientation_and_counts).reset_index(drop=True)\n",
    "abr_after = abr_data.shape[0]\n",
    "\n",
    "sixteen_s_data = minsim[(minsim['part'] == '16S')]\n",
    "sixteen_s_before = sixteen_s_data.shape[0]\n",
    "sixteen_s_data = sixteen_s_data.copy()\n",
    "sixteen_s_data[\"query_id\"] = sixteen_s_data[\"query_id\"].str.split(expand=True)[0]\n",
    "sixteen_s_data = sixteen_s_data.groupby(\"query_id\").apply(process_orientation_and_counts).reset_index(drop=True)\n",
    "sixteen_s_after = sixteen_s_data.shape[0]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### 2. Keep maximum percentage hit per query and datapart (16S/ABR):\\n\n",
    "- Dataframe is split in dataparts (ABR and 16S): ```abr = f_df[f_df['part'] == ABR]```\\n\n",
    "- Dataparts are grouped by query_id and processed: ```f_df_max = abr.groupby('query_id').apply(give_max_hit)```\\n\n",
    "Datapart \\'ABR\\' before reduction: {abr_before} vs after reduction: **{abr_after}** (reduction of {abr_before-abr_after})\\n\n",
    "Datapart \\'16S\\' before reduction: {sixteen_s_before} vs after reduction: **{sixteen_s_after}** (reduction of {sixteen_s_before-sixteen_s_after})\\n\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "common_query_ids = pd.Index(abr_data['query_id']).intersection(sixteen_s_data['query_id'])\n",
    "abr_data_filtered = abr_data[abr_data['query_id'].isin(common_query_ids)]\n",
    "sixteen_s_data_filtered = sixteen_s_data[sixteen_s_data['query_id'].isin(common_query_ids)]\n",
    "remerged_df = pd.concat([abr_data_filtered, sixteen_s_data_filtered])\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Step 3: Keep only intersectional hits\n",
    "- Common query IDs found between ABR and 16S.\n",
    "- Filtered data merged into a final dataset:\\n\n",
    "  ```common_hits = pd.Index(abr_data['query_id']).intersection(sixteen_s_data['query_id'])```\\n\n",
    "  ```abr_common = abr_data[abr_data['query_id'].isin(common)]```\\n\n",
    "  ```sixteen_s_common = sixteen_s_data[sixteen_s_data['query_id'].isin(common)]```\\n\n",
    "Datapart \\'ABR\\' before reduction: {abr_after} vs after reduction: **{abr_data_filtered.shape[0]}** (reduction of {abr_after-abr_data_filtered.shape[0]})\\n\n",
    "Datapart \\'16S\\' before reduction: {sixteen_s_after} vs after reduction: **{sixteen_s_data_filtered.shape[0]}** (reduction of {sixteen_s_after-sixteen_s_data_filtered.shape[0]})\\n\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "part_ABR = abr_data_filtered.head(1)\n",
    "part_16S = sixteen_s_data_filtered.head(1)\n",
    "summary = remerged_df.agg({\"perc_identity\": [\"mean\", \"min\", \"max\"],\"align_length\": [\"mean\", \"min\", \"max\"],\"evalue\": [\"mean\", \"min\", \"max\"]}).style.format({\"perc_identity\": \"{:.2f}\",\"align_length\": \"{:.0f}\",\"evalue\": \"{:.2e}\"})\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Final step: remerge filtered dataparts ABR/16S\\n\n",
    "```final_df = pd.concat([abr_common, sixteen_s_common])```\\n\n",
    "Filtered results before reduction: {int_df.shape[0]} vs after reduction: **{remerged_df.shape[0]}** (reduction of {int_df.shape[0]-remerged_df.shape[0]})\\n\n",
    "---\n",
    "### -> filtered_result.csv\\n\n",
    "\"\"\"))\n",
    "\n",
    "display(summary)\n",
    "display(pd.concat([part_16S, part_ABR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General statistics of filtered vs integrated intermediate results (BLASTX and usearch)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Statistics of all integrated and filtered_files from Rozman (blastn and usearch)\n",
    "- 12 Fasta files with 10 splits each\n",
    "- similarity search blastx/blastn and blastx/usearch\n",
    "data gathered with a bash for-loop:\\n\n",
    "```for i in *L001; do for j in $i/0*; do echo -ne \"$j,$(cat $j/filtered_results.csv|wc -l),$(du -hs $j/filtered_results.csv)\\\\n\" >> filtered_results_size.txt;done;done```\\n\n",
    "```for i in *L001; do for j in $i/0*; do echo -ne \"$j,$(cat $j/integrated_filtered_results.csv|wc -l),$(du -hs $j/integrated_filtered_results.csv)\\\\n\" >> integrated_filtered_results_size.txt;done;done```\\n\n",
    "plus 10 seconds manual change in the file preparing it as pandas input\n",
    "\"\"\"))\n",
    "\n",
    "# Define paths\n",
    "filtered_path = os.path.join(path, \"results_rozman\", \"filtered_results_size.txt\")\n",
    "integrated_path = os.path.join(path, \"results_rozman\", \"integrated_filtered_results_size.txt\")\n",
    "filtered_path_us = os.path.join(path, \"results_rozman\", \"filtered_results_size_rozman10-17_usearch.txt\")\n",
    "integrated_path_us = os.path.join(path, \"results_rozman\", \"integrated_filtered_results_size_rozman10-17_usearch.txt\")\n",
    "\n",
    "# Function to convert sizes to GB\n",
    "def convert_size_to_gb(size):\n",
    "    if size.endswith(\"K\"):\n",
    "        return float(size.replace(\"K\", \"\")) / 1000000  # Convert MB to GB\n",
    "    elif size.endswith(\"M\"):\n",
    "        return float(size.replace(\"M\", \"\")) / 1000  # Convert MB to GB\n",
    "    elif size.endswith(\"G\"):\n",
    "        return float(size.replace(\"G\", \"\"))  # Already in GB\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected size format: {size}\")\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=0)\n",
    "    df[\"lines\"] = df[\"lines\"].replace(\",\", \"\", regex=True).astype(int)  # Ensure numeric\n",
    "    df[\"size_GB\"] = df[\"size\"].apply(convert_size_to_gb)\n",
    "    summary_per_file = df.groupby(\"fasta-file\").agg(\n",
    "        total_lines=(\"lines\", \"sum\"),\n",
    "        total_size_GB=(\"size_GB\", \"sum\"),\n",
    "        average_lines=(\"lines\", \"mean\"),\n",
    "        average_size_GB=(\"size_GB\", \"mean\"),\n",
    "    )\n",
    "    overall_summary = {\n",
    "        \"Total Lines (All Files)\": df[\"lines\"].sum(),\n",
    "        \"Total Size (All Files) (GB)\": df[\"size_GB\"].sum(),\n",
    "        \"Average Lines per Fasta File\": summary_per_file[\"total_lines\"].mean(),\n",
    "        \"Average Size per Fasta File (GB)\": summary_per_file[\"total_size_GB\"].mean(),\n",
    "    }\n",
    "    return summary_per_file, overall_summary\n",
    "\n",
    "# Process both files\n",
    "filtered_summary, filtered_overall = process_file(filtered_path)\n",
    "integrated_summary, integrated_overall = process_file(integrated_path)\n",
    "filtered_summary_us, filtered_overall_us = process_file(filtered_path_us)\n",
    "integrated_summary_us, integrated_overall_us = process_file(integrated_path_us)\n",
    "\n",
    "# Combine summaries for comparison\n",
    "combined_summary = pd.concat(\n",
    "    [filtered_summary, integrated_summary],\n",
    "    axis=1,\n",
    "    keys=[\"Filtered\", \"Integrated\"]\n",
    ")\n",
    "\n",
    "combined_summary2 = pd.concat(\n",
    "    [filtered_summary_us, integrated_summary_us],\n",
    "    axis=1,\n",
    "    keys=[\"Filtered_usearch\", \"Integrated_usearch\"]\n",
    ")\n",
    "\n",
    "# Format combined summary\n",
    "for col in combined_summary.columns:  # Iterate over all columns\n",
    "    if \"lines\" in col[1]:\n",
    "        combined_summary[col] = combined_summary[col].map('{:,.0f}'.format)\n",
    "    elif \"size_GB\" in col[1]:\n",
    "        combined_summary[col] = combined_summary[col].map('{:.2f}'.format)\n",
    "    else:\n",
    "        print(col,\"not found\")\n",
    "\n",
    "for col in combined_summary2.columns:  # Iterate over all columns\n",
    "    if \"lines\" in col[1]:\n",
    "        combined_summary2[col] = combined_summary2[col].map('{:,.0f}'.format)\n",
    "    elif \"size_GB\" in col[1]:\n",
    "        combined_summary2[col] = combined_summary2[col].map('{:.4f}'.format)\n",
    "    else:\n",
    "        print(col,\"not found\")\n",
    "\n",
    "\n",
    "# Combine overall summaries for comparison\n",
    "combined_overall = {\n",
    "    \"Filtered\": filtered_overall,\n",
    "    \"Integrated\": integrated_overall,\n",
    "}\n",
    "\n",
    "combined_overall2 = {\n",
    "    \"Filtered\": filtered_overall_us,\n",
    "    \"Integrated\": integrated_overall_us,\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Combined Summary per Fasta File witch blastn:\")\n",
    "display(combined_summary)\n",
    "\n",
    "print(\"Combined Summary per Fasta File with usearch:\")\n",
    "display(combined_summary2)\n",
    "\n",
    "print(\"\\nOverall Summary Comparison with blastn:\")\n",
    "for key in combined_overall[\"Filtered\"]:\n",
    "    print(f\"Filtered:\\t{key}:\\t{combined_overall['Filtered'][key]:,.2f}\")\n",
    "    print(f\"Integrated:\\t{key}:\\t{combined_overall['Integrated'][key]:,.2f}\")\n",
    "\n",
    "print(\"\\nOverall Summary Comparison with usearch:\")\n",
    "for key in combined_overall2[\"Filtered\"]:\n",
    "    print(f\"Filtered:\\t{key}:\\t{combined_overall2['Filtered'][key]:,.2f}\")\n",
    "    print(f\"Integrated:\\t{key}:\\t{combined_overall2['Integrated'][key]:,.2f}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create abundance table and plot abundance\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: abundance plot over all ABRs\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "input_files = [\"/local/work/adrian/ERMA/results/test_epic_data/001/filtered_results.csv\"]\n",
    "necessary_columns = [\"query_id\",\"part\",\"genus\",\"AMR Gene Family\"]\n",
    "sample_name = \"test_epic_data\"\n",
    "\n",
    "df = pd.read_csv(input_files[0], sep=\",\", header=0)\n",
    "first_query = df[\"query_id\"].iloc[1]\n",
    "test = df[df[\"query_id\"] == first_query]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Create abundance table\n",
    "---\n",
    "### 1. Parse filtered results with reduced columns, drop duplicates and merge (see cell before)\n",
    "### 2. Merge split fasta parts in one dataframe\\n\n",
    "### 3. Groupy by AMR Gene Family and genus (export table)\\n\n",
    "```combined_df.groupby(['AMR Gene Family_abr', 'genus_16S']).size()```\\n\n",
    "### 4. Plot count per AMR Gene Family\n",
    "---\n",
    "## Create abundance plot\n",
    "\"\"\"))\n",
    "\n",
    "all_data = []\n",
    "for input_file in input_files:\n",
    "        df = pd.read_csv(input_file, sep=\",\", usecols=necessary_columns, header=0)\n",
    "        df = df.drop_duplicates()\n",
    "        # Split and merge ABR and 16S data by query_id\n",
    "        abr = df[df[\"part\"] == \"ABR\"]\n",
    "        sixteen_s = df[df[\"part\"] == \"16S\"]\n",
    "        df_merged = pd.merge(abr, sixteen_s, on='query_id', suffixes=('_abr', '_16S'))\n",
    "\n",
    "        # Assign genus based on 16S path for each merged record\n",
    "        all_data.append(df_merged)  # Append the DataFrame to the list\n",
    "        # Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Group by AMR gene family and genus across all files\n",
    "genus_distribution = combined_df.groupby(['AMR Gene Family_abr', 'genus_16S']).size().reset_index(name='count')\n",
    "display(genus_distribution)\n",
    "# Create the Altair bar chart\n",
    "chart = alt.Chart(genus_distribution).mark_bar().encode(\n",
    "        x=alt.X('AMR Gene Family_abr', title='AMR Gene Family'),\n",
    "        y=alt.Y('count', title='Count'),\n",
    "        color=alt.Color('genus_16S', title='Genus'),\n",
    "        tooltip=['AMR Gene Family_abr', 'genus_16S', 'count']\n",
    ").properties(\n",
    "title=f'Genus Distribution for {sample_name}',\n",
    "width=800,\n",
    "height=400\n",
    ")\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 5. Create bubble plot\n",
    "# Selfwritten python script \"generate_genus_distribution_plot.py\"\n",
    "# Input: abundance file\n",
    "# Output: bubble plot per sample\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Create bubble plot\n",
    "---\n",
    "### 1. Parse abundance file\n",
    "### 2. Groupy by AMR Gene Family and genus (export table)\\n\n",
    "```combined_df.groupby(['AMR Gene Family_abr', 'genus_16S']).size()```\\n\n",
    "### 3. Reduce to ABR with most total hits\n",
    "### 4. Plot sample vs genus with relative genus count as size and total genus count as colour              \n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "def create_bubble_plots(df, abundance_threshold, output1,output2):\n",
    "    # Iterate over unique AMR Gene Families\n",
    "    # Filter data for the current AMR Gene Family\n",
    "    df = pd.read_csv(df,header=0,sep=',')\n",
    "    total_counts_per_abr = df.groupby('AMR Gene Family')['genus_count'].sum()\n",
    "    top_abr = total_counts_per_abr.idxmax()\n",
    "    top_abr_data = df[df['AMR Gene Family'] == top_abr]\n",
    "    top_abr_data = top_abr_data[top_abr_data['relative_genus_count'] > float(abundance_threshold)]\n",
    "    # Create the bubble plot\n",
    "    fig = px.scatter(\n",
    "        top_abr_data, \n",
    "        x=\"sample\", \n",
    "        y=\"genus\", \n",
    "        size=\"relative_genus_count\",\n",
    "        color=\"total_genus_count\", \n",
    "        hover_name=\"genus\",\n",
    "        hover_data={\n",
    "            \"genus_count\": True,\n",
    "            \"relative_genus_count\": True,\n",
    "            \"total_genus_count\": True,\n",
    "            \"sample\": False\n",
    "        },\n",
    "        size_max=20,\n",
    "        color_continuous_scale=\"Greens\"\n",
    "    )\n",
    "    \n",
    "    # Update layout for titles and axis labels\n",
    "    fig.update_layout(\n",
    "        title=f'Bubble Plot of Relative Genera Abundance per Sample and top found AMR:<br> {top_abr} with {total_counts_per_abr.sum()} fusion reads over all samples',\n",
    "        xaxis_title='Sample - AMR Gene Family',\n",
    "        yaxis_title='Genus',\n",
    "        coloraxis_colorbar=dict(title=\"Total Genus Count\"),\n",
    "        #paper_bgcolor='grey',\n",
    "        plot_bgcolor='lightgrey',\n",
    "        yaxis=dict(categoryorder=\"category descending\")\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = \"/local/work/adrian/ERMA/results/abundance/combined_genus_abundance.csv\"\n",
    "    output_html = \"snakemake.output[0]\"\n",
    "    output_csv = \"snakemake.output[1]\"\n",
    "    abundance_threshold = 0.001\n",
    "    create_bubble_plots(input_files, abundance_threshold, output_html,output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create boxplots\n",
    "# Selfwritten python script \"percidt_per_genus.py\"\n",
    "# Input: all filtered_result.csv parts of one sample\n",
    "# Output: boxplot over all samples per percentage identity, number of unique hits and genera\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Create boxplot\n",
    "---\n",
    "### 1. Parse filtered results with reduced columns, drop duplicates and merge (see cell before)\n",
    "### 2. Merge split fasta parts in one dataframe\\n\n",
    "### 3. Groupy by AMR Gene Family and genus (export table)\\n\n",
    "```combined_df.groupby(['AMR Gene Family_abr', 'genus_16S']).size()```\\n\n",
    "### 4. Plot count per AMR Gene Family\n",
    "---\n",
    "## Plot abundance\n",
    "\"\"\"))\n",
    "\n",
    "def generate_percentage_idt_per_genus(input_files, output_file):\n",
    "        \n",
    "    # Combine all partitions into a single DataFrame\n",
    "    combined_data = pd.read_csv(input_files, sep=\",\", header=0)\n",
    "    \n",
    "    # Calculate genus query counts and genus order\n",
    "    genus_query_counts = combined_data.groupby('genus')['query_id'].nunique().reset_index()\n",
    "    genus_query_counts.columns = ['genus', 'unique_query_count']\n",
    "    combined_data = pd.merge(combined_data, genus_query_counts, on='genus')\n",
    "    genus_order = genus_query_counts.sort_values(by='unique_query_count', ascending=False)['genus']\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    sns.boxplot(x='genus', y='perc_identity', data=combined_data, ax=ax1, order=genus_order,fliersize=0.0, color='dodgerblue')\n",
    "    ax1.set_xlabel(\"Bacterial Genus\")\n",
    "    ax1.set_ylabel(\"Percentage Identity\", color='royalblue')\n",
    "    ax1.set_title(\"Boxplot of Percentage Identity and Read Counts for Each Bacterial genus\")\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Add a second y-axis for unique query counts\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.barplot(x='genus', y='unique_query_count', data=genus_query_counts, ax=ax2, alpha=0.3, color='purple', order=genus_order)\n",
    "    ax2.set_ylabel(\"Number of Unique Reads (query_id)\", color='violet')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = \"/local/work/adrian/ERMA/results/test_epic_data/001/filtered_results.csv\"\n",
    "    output_file = \"snakemake.output[0]\"\n",
    "    generate_percentage_idt_per_genus(input_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERMA_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
